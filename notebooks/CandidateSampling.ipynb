{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PARQUET_PATH = \"/home/gmoreira/dataset/ecommerce_preproc_2019-*/ecommerce_preproc.parquet/session_start_date=*\"\n",
    "OUTPUT_NEG_SAMPLES_PARQUET_PATH = \"/home/gmoreira/dataset/neg_samples.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIFORM_SAMPLING = 'uniform'\n",
    "RECENCY_SAMPLING = 'recency'\n",
    "RECENT_POPULARITY_SAMPLING = 'popularity'\n",
    "COOCURRENCE_SAMPLING = 'cooccurrence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_SAMPLING_STRATEGY = COOCURRENCE_SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "BATCHES_TO_UPDATE_ITEM_STATS = 3\n",
    "BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES = 5\n",
    "ITEM_STATS_KEEP_LAST_N_DAYS = 1.0\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_NEG_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-30',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-31',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-30']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_parquet_files = sorted(glob.glob(INPUT_PARQUET_PATH+'*'))\n",
    "input_parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path_parquet_neg_samples(input_parquet_filename):\n",
    "    return input_parquet_filename \\\n",
    "        .replace('ecommerce_preproc.parquet', 'ecommerce_preproc_neg_samples_{}_strategy_{}.parquet' \\\n",
    "                     .format(NUM_NEG_SAMPLES, NEGATIVE_SAMPLING_STRATEGY)) + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_files(data_paths):\n",
    "    paths = [['file://' + p for p in glob.glob(path + \"/*.parquet\")] for path in data_paths]\n",
    "    return list(itertools.chain.from_iterable(paths))\n",
    "input_parquet_files = get_files([INPUT_PARQUET_PATH])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Works but cannot be used now because the preprocessed sessions are not sorted by timestamp\n",
    "def read_parquet_generator(filenames, batch_size=128):\n",
    "    for filename in filenames:\n",
    "        for batch in pq.read_table(filename).to_batches(batch_size):\n",
    "            yield batch.to_pandas()\n",
    "            \n",
    "parquet_reader = read_parquet_generator([INPUT_PARQUET_PATH], batch_size=BATCH_SIZE)            \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_into_chuncks_generator(df, chunk_size): \n",
    "    number_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(number_chunks):\n",
    "        yield df[i*chunk_size:(i+1)*chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_update_session_items_metadata(row):\n",
    "    #Uses the session start as the event timestamp (as the sess_etime_seq might sometimes be many days before because of outlier sessions with more than 120 min duration (< 1%))\n",
    "    etime = row['session_start_ts']\n",
    "    #For each session\n",
    "    for pid, csid, ccid, bid, price, relative_price, prod_recency in zip(\n",
    "                                                        #row['sess_etime_seq'],\n",
    "                                                        row['sess_pid_seq'], \n",
    "                                                        row['sess_csid_seq'],\n",
    "                                                        row['sess_ccid_seq'],\n",
    "                                                        row['sess_bid_seq'],\n",
    "                                                        row['sess_price_seq'],\n",
    "                                                        row['sess_relative_price_to_avg_category_seq'],\n",
    "                                                        row['sess_product_recency_seq']):\n",
    "\n",
    "        #If this item was not processed before\n",
    "        if pid != 0:\n",
    "            if pid in items_df.index:\n",
    "                curr_row = items_df.loc[pid]\n",
    "\n",
    "                first_ts = curr_row['first_ts']\n",
    "                last_ts = curr_row['last_ts']\n",
    "                if etime > last_ts:\n",
    "                    last_ts = etime\n",
    "            else:\n",
    "                first_ts = etime\n",
    "                last_ts = etime\n",
    "\n",
    "            #Including or updating the item metadata\n",
    "            items_df.loc[pid] = pd.Series({'csid': csid,\n",
    "                                           'ccid': ccid,\n",
    "                                           'bid': bid,\n",
    "                                           'price': price,\n",
    "                                           'relative_price_to_avg_category': relative_price,\n",
    "                                           'product_recency': prod_recency,\n",
    "                                           'first_ts': first_ts,\n",
    "                                           'last_ts': last_ts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_item_coocurrences_log(row):\n",
    "    global items_coocurrence_df\n",
    "    min_ts = min([t for t in row['sess_etime_seq'] if t != 0])\n",
    "    valid_pids = list(set(list([p for p in row['sess_pid_seq'] if p != 0])))\n",
    "    \n",
    "    if len(valid_pids) > 1:\n",
    "        items_permutations = permutations(valid_pids, 2)        \n",
    "        new_coo_df = pd.DataFrame(items_permutations, columns=['pid_a', 'pid_b'])\n",
    "        new_coo_df['ts'] = min_ts\n",
    "        #This flag is used for counting unique values from this table to compute popularity\n",
    "        new_coo_df['count_flag'] = ([1] + [0]*(len(valid_pids)-2))*len(valid_pids)\n",
    "        \n",
    "        items_coocurrence_df = pd.concat([items_coocurrence_df, new_coo_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_old_interactions(keep_last_n_days):\n",
    "    global items_coocurrence_df\n",
    "    last_ts = items_coocurrence_df['ts'].max()\n",
    "    keep_last_n_secs = keep_last_n_days * 24 * 60 * 60\n",
    "    return items_coocurrence_df[items_coocurrence_df['ts'] >= (last_ts - keep_last_n_secs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_temporal_relevance_decay():    \n",
    "    global items_temporal_relev_df\n",
    "    max_reference_ts = items_df['first_ts'].max()\n",
    "    prods_days_age = (max_reference_ts - items_df['first_ts']) / (60 * 60 * 24)\n",
    "\n",
    "    time_relev_by_item_series = prod_relevance_decay(prods_days_age)\n",
    "    items_temporal_relev_df = time_relev_by_item_series / time_relev_by_item_series.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_coocurrences_counts():\n",
    "    global items_coocurence_counts_df\n",
    "    items_coocurence_counts_df = items_coocurrence_df.groupby(['pid_a','pid_b']).size().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_recent_popularity():\n",
    "    global items_recent_pop_df\n",
    "    items_recent_pop_df = items_coocurrence_df[items_coocurrence_df['count_flag'] == True] \\\n",
    "            .groupby(['pid_a']).size().to_frame('count')\n",
    "    items_recent_pop_df['prob'] = items_recent_pop_df['count'] / items_recent_pop_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "\n",
    "def prod_relevance_decay(days_age):\n",
    "    return np.exp(-days_age*DAYS_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "30 0.9417645335842487\n",
      "60 0.8869204367171575\n",
      "90 0.835270211411272\n",
      "120 0.7866278610665535\n",
      "150 0.7408182206817179\n",
      "180 0.697676326071031\n",
      "210 0.6570468198150567\n",
      "240 0.6187833918061408\n",
      "270 0.5827482523739896\n",
      "300 0.5488116360940264\n",
      "330 0.5168513344916992\n",
      "360 0.4867522559599717\n",
      "390 0.4584060113052235\n",
      "420 0.43171052342907973\n",
      "450 0.4065696597405991\n",
      "480 0.38289288597511206\n",
      "510 0.3605949401730783\n",
      "540 0.3395955256449391\n",
      "570 0.31981902181630384\n",
      "600 0.30119421191220214\n",
      "630 0.2836540264997704\n",
      "660 0.26713530196585034\n",
      "690 0.25157855305975646\n",
      "720 0.23692775868212176\n"
     ]
    }
   ],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "# Simulating 2 year of decay on relevance of a product \n",
    "for i in np.arange(0,365*2,30):    \n",
    "    print(i, prod_relevance_decay(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = None\n",
    "items_coocurrence_df = None\n",
    "items_coocurence_counts_df = None\n",
    "items_recent_pop_df = None\n",
    "items_temporal_relev_df = None\n",
    "\n",
    "def reset_item_logs_and_statistics():\n",
    "    global items_df, items_coocurrence_df, items_coocurence_counts_df, items_recent_pop_df, items_temporal_relev_df\n",
    "    \n",
    "    items_df = pd.DataFrame(columns={'pid': np.int64,\n",
    "                                 'csid': np.int32,\n",
    "                                 'ccid': np.int32,\n",
    "                                 'bid': np.int32,\n",
    "                                 'price': np.float,\n",
    "                                 'relative_price_to_avg_category': np.float,\n",
    "                                 'product_recency': np.float,\n",
    "                                 'first_ts': np.int,\n",
    "                                 'last_ts': np.int\n",
    "                                }).set_index('pid')\n",
    "    \n",
    "    items_coocurrence_df = pd.DataFrame(columns={'pid_a': np.int64, \n",
    "                                                 'pid_b': np.int64, \n",
    "                                                 'ts': np.int32, \n",
    "                                                 'count_flag': np.int16})\n",
    "    \n",
    "    items_coocurence_counts_df = None\n",
    "    items_recent_pop_df = None\n",
    "    items_temporal_relev_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_df.index, min(n_samples, len(items_df)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_popularity_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_recent_pop_df.index, min(n_samples, len(items_recent_pop_df)), replace=False, \n",
    "                            p=items_recent_pop_df['prob']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coocurrence_sampling_item_ids(pid, n_samples):\n",
    "    samples = []\n",
    "    if pid in items_coocurence_counts_df.index:\n",
    "        coocurrent = items_coocurence_counts_df.loc[pid]\n",
    "        coocurrent_probs = coocurrent['count'] / coocurrent['count'].sum()\n",
    "        samples = np.random.choice(coocurrent_probs.index, min(n_samples, len(coocurrent_probs)), replace=False, \n",
    "                                   p=coocurrent_probs.values).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recency_sampling_item_ids(n_samples):\n",
    "    samples = np.random.choice(items_temporal_relev_df.index, min(n_samples, len(items_temporal_relev_df)), replace=False, \n",
    "                               p=items_temporal_relev_df.values).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_samples_item_ids(pid, n_samples, strategy, ignore_list=None):\n",
    "    #To ensure that after removing sessions from the current session we have the required number of samples\n",
    "    SAMPLES_MULITPLIER = 2\n",
    "    if strategy == UNIFORM_SAMPLING:\n",
    "        samples = get_uniform_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENCY_SAMPLING:\n",
    "        samples = get_recency_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENT_POPULARITY_SAMPLING:\n",
    "        samples = get_popularity_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)    \n",
    "    elif strategy == COOCURRENCE_SAMPLING:\n",
    "        samples = get_coocurrence_sampling_item_ids(pid, n_samples*SAMPLES_MULITPLIER)\n",
    "    \n",
    "        #Completing the list of samples based on global popularity\n",
    "        if len(samples) < n_samples:\n",
    "            samples += get_popularity_sampling_item_ids(n_samples - len(samples))\n",
    "    else:\n",
    "        raise Exception('Not a valid strategy. Should be: (uniform|recency|popularity|cooccurrence)')\n",
    "        \n",
    "    #Removing repeated entries\n",
    "    samples = list(set(samples))\n",
    "    if ignore_list is not None:\n",
    "        samples = list([i for i in samples if i not in ignore_list])\n",
    "            \n",
    "    return samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_candidate_samples_item_ids(63246, 10, strategy=COOCURRENCE_SAMPLING, ignore_list=[2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_item_ids(pids):\n",
    "    return items_df.loc[pids][['csid', 'ccid', 'bid', 'price', 'relative_price_to_avg_category', 'product_recency']] \\\n",
    "    .to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_features_for_item_ids([2010, 63246])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padarray(A, size):\n",
    "    if len(A) > size:\n",
    "        A = A[:size]\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padarray([1,2,3], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_samples(session_pids, user_past_pids, n_samples, strategy):\n",
    "    neg_samples_dict = defaultdict(list)\n",
    "    \n",
    "    #Ignores session items and also recently interacted items\n",
    "    ignore_ids = set(np.hstack([session_pids, user_past_pids]))\n",
    "    \n",
    "    for pid in session_pids:\n",
    "        if pid != 0:\n",
    "            neg_item_ids = get_candidate_samples_item_ids(pid, n_samples, \n",
    "                                                          ignore_list=ignore_ids,\n",
    "                                                         strategy=strategy)\n",
    "            neg_item_features_dict = get_features_for_item_ids(neg_item_ids)\n",
    "\n",
    "            pids_padded = padarray(neg_item_ids, n_samples).astype(int)\n",
    "            neg_samples_dict['sess_neg_pids'].append(pids_padded)\n",
    "            for k, v in neg_item_features_dict.items():\n",
    "                values = padarray(v, n_samples)\n",
    "                values = values.astype(int) if k in ['csid', 'ccid', 'bid'] else values\n",
    "                neg_samples_dict['sess_neg_{}'.format(k)].append(values)\n",
    "        else:\n",
    "            #Creating padding neg samples for each padding interactions\n",
    "            missing_padding_neg_samples = len(session_pids) - len(neg_samples_dict['sess_neg_pids'])            \n",
    "            for p in range(missing_padding_neg_samples):\n",
    "                for k in neg_samples_dict:\n",
    "                    #Copying shape and dtype from the neg samples of the first interaction\n",
    "                    neg_samples_dict[k].append(np.zeros_like(neg_samples_dict[k][0]))\n",
    "              \n",
    "    #Concatenating neg. samples of all session interactions because Petastorm data loader \n",
    "    #does not support lists of lists. It will require reshaping neg. samples features inside the Pytorch model\n",
    "    for k in neg_samples_dict:  \n",
    "        neg_samples_dict[k] = np.hstack(neg_samples_dict[k])        \n",
    "\n",
    "    return neg_samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_neg_samples([10,20,30, 0, 0], n_samples=2, strategy='popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_rows_to_parquet(new_rows_df, path):\n",
    "    global pq_writer\n",
    "    new_rows_pa = pyarrow.Table.from_pandas(new_rows_df)\n",
    "    if pq_writer is None:\n",
    "        #Creating parent folder recursively\n",
    "        parent_folder = os.path.dirname(os.path.abspath(path))\n",
    "        if not os.path.exists(parent_folder):\n",
    "            os.makedirs(parent_folder)\n",
    "        #Creating parquet file\n",
    "        pq_writer = pq.ParquetWriter(path, new_rows_pa.schema) \n",
    "    pq_writer.write_table(new_rows_pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates neg. samples for all sessions and creates new parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "[Day 0] Loading sessions from parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:11, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:31, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:54, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:20, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3] Updating item stats\n",
      "[Batch 4] Updating item stats\n",
      "[Batch 4] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [02:23, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 5] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [03:34, 30.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 8] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [04:13, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 9] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [05:42, 38.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 11] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [08:10, 46.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 14] Updating item stats\n",
      "[Batch 14] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [10:59, 53.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 17] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [13:04, 57.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 19] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [14:12, 60.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 20] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [15:24, 64.25s/it]"
     ]
    }
   ],
   "source": [
    "pq_writer = None\n",
    "\n",
    "reset_item_logs_and_statistics()\n",
    "try:\n",
    "    #For each file (day)\n",
    "    for idx_day, input_file in enumerate(input_parquet_files):\n",
    "        print('='*40)\n",
    "        print('[Day {}] Loading sessions from parquet: {}'.format(idx_day, input_file))\n",
    "        output_filename = get_output_path_parquet_neg_samples(input_file)\n",
    "        \n",
    "        if os.path.exists(output_filename):\n",
    "            raise Exception('Output parquet file already exists')\n",
    "        \n",
    "        #Loading parquet file and sorting sessions by timestamp\n",
    "        sessions_df = pd.read_parquet(input_file)\n",
    "        sessions_df.sort_values('session_start_ts', inplace=True)\n",
    "                \n",
    "        new_rows = []\n",
    "        \n",
    "        #For each batch\n",
    "        for batch_id, batch in tqdm(enumerate(split_dataframe_into_chuncks_generator(sessions_df, \n",
    "                                                                                chunk_size = BATCH_SIZE))):            \n",
    "            #For each row (session)\n",
    "            for i, row in batch.iterrows():\n",
    "                insert_update_session_items_metadata(row)\n",
    "                update_item_coocurrences_log(row)\n",
    "                \n",
    "                #Ignoring first batch (not computing neg. samples nor saving to parquet)\n",
    "                if batch_id > 0:                    \n",
    "                    #Generating neg. samples for each interaction in the session\n",
    "                    session_neg_samples_by_pid_dict = generate_neg_samples(row['sess_pid_seq'], \n",
    "                                                                           row['user_pid_seq_bef_sess'],\n",
    "                                                                           NUM_NEG_SAMPLES, \n",
    "                                                                           strategy=NEGATIVE_SAMPLING_STRATEGY)\n",
    "                    #Merging user and session features with neg samples for the session\n",
    "                    new_row_with_neg_samples_dict = {**row.to_dict(), **session_neg_samples_by_pid_dict}\n",
    "                    new_rows.append(new_row_with_neg_samples_dict)\n",
    "\n",
    "            #Each N batches updates item statistics (popularity, recency, co-occurrence)\n",
    "            #Ps. Do the update for all the first five batches of the first file , for better sampling\n",
    "            if (batch_id % BATCHES_TO_UPDATE_ITEM_STATS == BATCHES_TO_UPDATE_ITEM_STATS-1) or \\\n",
    "               (idx_day == 0 and batch_id < 5):\n",
    "                print('[Batch {}] Updating item stats'.format(batch_id))\n",
    "                remove_old_interactions(ITEM_STATS_KEEP_LAST_N_DAYS)\n",
    "                if NEGATIVE_SAMPLING_STRATEGY in [RECENT_POPULARITY_SAMPLING, COOCURRENCE_SAMPLING]:\n",
    "                    update_items_coocurrences_counts()\n",
    "                    if RECENT_POPULARITY_SAMPLING:\n",
    "                        update_items_recent_popularity()\n",
    "                if NEGATIVE_SAMPLING_STRATEGY == RECENCY_SAMPLING:\n",
    "                    update_items_temporal_relevance_decay()\n",
    "                \n",
    "            #Each N batches appends the new rows with neg. samples to parquet file\n",
    "            if batch_id % BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES == BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES-1: \n",
    "                print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "                append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "                del(new_rows)\n",
    "                new_rows = []\n",
    "               \n",
    "        #Save pending rows\n",
    "        if len(new_rows) > 0:\n",
    "            print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "            append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "            del(new_rows)\n",
    "            new_rows = []\n",
    "            \n",
    "        #Flushing and releasing the current parquet file and proceeding for the new date\n",
    "        pq_writer.close()\n",
    "        pq_writer = None\n",
    "                \n",
    "        del(sessions_df)\n",
    "        gc.collect()\n",
    "        \n",
    "        break\n",
    "finally:\n",
    "    if pq_writer:\n",
    "        pq_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the parquet with Negative samples with Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.pytorch import DataLoader\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.unischema import UnischemaField\n",
    "from petastorm.unischema import Unischema\n",
    "from petastorm.codecs import NdarrayCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_neg_parquet_path = 'file:///home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_schema_full = [\n",
    "  UnischemaField('user_idx', np.int, (), None, True),\n",
    "#   UnischemaField('user_session', str_, (), None, True),\n",
    "  UnischemaField('sess_seq_len', np.int, (), None, False),\n",
    "  UnischemaField('session_start_ts', np.int64, (), None, True),\n",
    "  UnischemaField('user_seq_length_bef_sess', np.int, (), None, False),\n",
    "  UnischemaField('user_elapsed_days_bef_sess', np.float, (), None, True),\n",
    "  UnischemaField('user_elapsed_days_log_bef_sess_norm', np.double, (), None, True),\n",
    "  UnischemaField('sess_pid_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etime_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etype_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_csid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_ccid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_bid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_price_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_dtime_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_product_recency_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_relative_price_to_avg_category_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('user_pid_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etime_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etype_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_csid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_ccid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_bid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_price_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_dtime_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_product_recency_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_relative_price_to_avg_category_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_pids', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_csid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_ccid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_bid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_price', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_relative_price_to_avg_category', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_product_recency', np.float, (None,), None, True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0\n",
      "{'user_idx': tensor([534142828, 518780152]), 'sess_seq_len': tensor([6, 3]), 'session_start_ts': tensor([1569896980, 1569896981]), 'user_seq_length_bef_sess': tensor([0, 0]), 'user_elapsed_days_bef_sess': tensor([nan, nan], dtype=torch.float64), 'user_elapsed_days_log_bef_sess_norm': tensor([nan, nan], dtype=torch.float64), 'sess_pid_seq': tensor([[    2,   100,    41,   146,   286,     4,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1845,  6618, 12197,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'sess_etime_seq': tensor([[1569896980, 1569897049, 1569897071, 1569897139, 1569897196, 1569897469,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0],\n",
      "        [1569896981, 1569897044, 1569897171,          0,          0,          0,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0]]), 'sess_etype_seq': tensor([[2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'sess_csid_seq': tensor([[  2,   2,   2,   2,   2,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [127, 372, 372,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]], dtype=torch.int32), 'sess_ccid_seq': tensor([[  2,   2,   2,   2,   2,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [131, 131, 131,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]], dtype=torch.int32), 'sess_bid_seq': tensor([[   2,    5,    4,    4,    5,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  81,  115, 1603,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int32), 'sess_price_seq': tensor([[-0.1304,  0.1173, -0.3239, -0.1124, -0.1292,  0.4140,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4106, -1.1723, -1.4400,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_dtime_seq': tensor([[-0.0230, -0.0161, -0.0208, -0.0162, -0.0173, -0.0173,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0230, -0.0167, -0.0103,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_product_recency_seq': tensor([[-3.7406, -3.8512, -3.7435, -3.7497, -3.7822, -3.7341,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-3.8482, -3.8576, -3.8575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_relative_price_to_avg_category_seq': tensor([[-0.7269, -0.6299, -0.7847, -0.7208, -0.7265, -0.4678,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4729, -0.3859, -0.5616,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_hour_sin_seq': tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_hour_cos_seq': tensor([[0.8660, 0.8660, 0.8660, 0.8660, 0.8660, 0.8660, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.8660, 0.8660, 0.8660, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_month_sin_seq': tensor([[-0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8660, -0.8660, -0.8660,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_month_cos_seq': tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofweek_sin_seq': tensor([[0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.4339, 0.4339, 0.4339, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofweek_cos_seq': tensor([[-0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9010, -0.9010, -0.9010,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_dayofmonth_sin_seq': tensor([[0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.2013, 0.2013, 0.2013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofmonth_cos_seq': tensor([[0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.9795, 0.9795, 0.9795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'user_pid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'user_etime_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'user_etype_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_csid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_ccid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_bid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_price_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_dtime_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_product_recency_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_relative_price_to_avg_category_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_hour_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_hour_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_month_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_month_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofweek_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofweek_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofmonth_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofmonth_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'sess_neg_pids': tensor([[    3,     6,     7,  ...,     0,     0,     0],\n",
      "        [  641,     2, 19842,  ...,     0,     0,     0]]), 'sess_neg_csid': tensor([[  2,   2,   2,  ...,   0,   0,   0],\n",
      "        [  2,   2, 433,  ...,   0,   0,   0]]), 'sess_neg_ccid': tensor([[  2,   2,   2,  ...,   0,   0,   0],\n",
      "        [  2,   2, 131,  ...,   0,   0,   0]]), 'sess_neg_bid': tensor([[  3,   2,   2,  ...,   0,   0,   0],\n",
      "        [  2,   2, 115,  ...,   0,   0,   0]]), 'sess_neg_price': tensor([[ 1.5132,  0.5109,  0.1060,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.6895, -0.1304, -0.9275,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64), 'sess_neg_relative_price_to_avg_category': tensor([[ 1.0375, -0.4009, -0.6350,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.5261, -0.7269,  0.0480,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64), 'sess_neg_product_recency': tensor([[-3.7361, -3.7413, -3.7442,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-3.8343, -3.7406, -3.8576,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64)}\n",
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "with DataLoader(\n",
    "    make_batch_reader(input_with_neg_parquet_path, \n",
    "                num_epochs=1,\n",
    "                # transform_spec=transform\n",
    "                schema_fields=recsys_schema_full,\n",
    "    ), batch_size=2) as train_loader:\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(\"i:{}\".format(i))\n",
    "        print(batch)\n",
    "        print(batch['sess_neg_product_recency'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
