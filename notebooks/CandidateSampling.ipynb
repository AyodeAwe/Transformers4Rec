{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PARQUET_PATH = \"/home/gmoreira/dataset/ecommerce_preproc_2019-*/ecommerce_preproc.parquet/session_start_date=*\"\n",
    "OUTPUT_NEG_SAMPLES_PARQUET_PATH = \"/home/gmoreira/dataset/neg_samples.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIFORM_SAMPLING = 'uniform'\n",
    "RECENCY_SAMPLING = 'recency'\n",
    "RECENT_POPULARITY_SAMPLING = 'popularity'\n",
    "COOCURRENCE_SAMPLING = 'cooccurrence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_SAMPLING_STRATEGY = COOCURRENCE_SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "BATCHES_TO_UPDATE_ITEM_STATS = 3\n",
    "BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES = 5\n",
    "ITEM_STATS_KEEP_LAST_N_DAYS = 1.0\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_NEG_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-30',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-31',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-30']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_parquet_files = sorted(glob.glob(INPUT_PARQUET_PATH+'*'))\n",
    "input_parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path_parquet_neg_samples(input_parquet_filename):\n",
    "    return input_parquet_filename \\\n",
    "        .replace('ecommerce_preproc.parquet', 'ecommerce_preproc_neg_samples_{}_strategy_{}.parquet' \\\n",
    "                     .format(NUM_NEG_SAMPLES, NEGATIVE_SAMPLING_STRATEGY)) + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_files(data_paths):\n",
    "    paths = [['file://' + p for p in glob.glob(path + \"/*.parquet\")] for path in data_paths]\n",
    "    return list(itertools.chain.from_iterable(paths))\n",
    "input_parquet_files = get_files([INPUT_PARQUET_PATH])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Works but cannot be used now because the preprocessed sessions are not sorted by timestamp\n",
    "def read_parquet_generator(filenames, batch_size=128):\n",
    "    for filename in filenames:\n",
    "        for batch in pq.read_table(filename).to_batches(batch_size):\n",
    "            yield batch.to_pandas()\n",
    "            \n",
    "parquet_reader = read_parquet_generator([INPUT_PARQUET_PATH], batch_size=BATCH_SIZE)            \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_into_chuncks_generator(df, chunk_size): \n",
    "    number_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(number_chunks):\n",
    "        yield df[i*chunk_size:(i+1)*chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_update_session_items_metadata(row):\n",
    "    #Uses the session start as the event timestamp (as the sess_etime_seq might sometimes be many days before because of outlier sessions with more than 120 min duration (< 1%))\n",
    "    etime = row['session_start_ts']\n",
    "    #For each session\n",
    "    for pid, csid, ccid, bid, price, relative_price, prod_recency in zip(\n",
    "                                                        #row['sess_etime_seq'],\n",
    "                                                        row['sess_pid_seq'], \n",
    "                                                        row['sess_csid_seq'],\n",
    "                                                        row['sess_ccid_seq'],\n",
    "                                                        row['sess_bid_seq'],\n",
    "                                                        row['sess_price_seq'],\n",
    "                                                        row['sess_relative_price_to_avg_category_seq'],\n",
    "                                                        row['sess_product_recency_seq']):\n",
    "\n",
    "        #If this item was not processed before\n",
    "        if pid != 0:\n",
    "            if pid in items_df.index:\n",
    "                curr_row = items_df.loc[pid]\n",
    "\n",
    "                first_ts = curr_row['first_ts']\n",
    "                last_ts = curr_row['last_ts']\n",
    "                if etime > last_ts:\n",
    "                    last_ts = etime\n",
    "            else:\n",
    "                first_ts = etime\n",
    "                last_ts = etime\n",
    "\n",
    "            #Including or updating the item metadata\n",
    "            items_df.loc[pid] = pd.Series({'csid': csid,\n",
    "                                           'ccid': ccid,\n",
    "                                           'bid': bid,\n",
    "                                           'price': price,\n",
    "                                           'relative_price_to_avg_category': relative_price,\n",
    "                                           'product_recency': prod_recency,\n",
    "                                           'first_ts': first_ts,\n",
    "                                           'last_ts': last_ts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_cooccurrences_log_list = []\n",
    "\n",
    "def append_session_coocurrences_log(row):\n",
    "    global session_cooccurrences_log_list\n",
    "    min_ts = min([t for t in row['sess_etime_seq'] if t != 0])\n",
    "    valid_pids = list(set(list([p for p in row['sess_pid_seq'] if p != 0])))\n",
    "    \n",
    "    if len(valid_pids) > 1:\n",
    "        items_permutations = permutations(valid_pids, 2)        \n",
    "        new_coo_df = pd.DataFrame(items_permutations, columns=['pid_a', 'pid_b'])\n",
    "        new_coo_df['ts'] = min_ts\n",
    "        #This flag is used for counting unique values from this table to compute popularity\n",
    "        new_coo_df['count_flag'] = ([1] + [0]*(len(valid_pids)-2))*len(valid_pids)\n",
    "        \n",
    "        session_cooccurrences_log_list.append(new_coo_df)\n",
    "\n",
    "def concat_sessions_coocurrences_log():\n",
    "    global items_coocurrence_df, session_cooccurrences_log_list\n",
    "    items_coocurrence_df = pd.concat([items_coocurrence_df] + session_cooccurrences_log_list)\n",
    "    session_cooccurrences_log_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_old_interactions(keep_last_n_days):\n",
    "    global items_coocurrence_df\n",
    "    last_ts = items_coocurrence_df['ts'].max()\n",
    "    keep_last_n_secs = keep_last_n_days * 24 * 60 * 60\n",
    "    items_coocurrence_df = items_coocurrence_df[items_coocurrence_df['ts'] >= (last_ts - keep_last_n_secs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_temporal_relevance_decay():    \n",
    "    global items_temporal_relev_df\n",
    "    max_reference_ts = items_df['first_ts'].max()\n",
    "    prods_days_age = (max_reference_ts - items_df['first_ts']) / (60 * 60 * 24)\n",
    "\n",
    "    time_relev_by_item_series = prod_relevance_decay(prods_days_age)\n",
    "    items_temporal_relev_df = time_relev_by_item_series / time_relev_by_item_series.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_coocurrences_counts():\n",
    "    global items_coocurence_counts_df\n",
    "    items_coocurence_counts_df = items_coocurrence_df.groupby(['pid_a','pid_b']).size().to_frame('count') \\\n",
    "                                    .reset_index(level=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_recent_popularity():\n",
    "    global items_recent_pop_df\n",
    "    items_recent_pop_df = items_coocurrence_df[items_coocurrence_df['count_flag'] == True] \\\n",
    "            .groupby(['pid_a']).size().to_frame('count')\n",
    "    items_recent_pop_df['prob'] = items_recent_pop_df['count'] / items_recent_pop_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "\n",
    "def prod_relevance_decay(days_age):\n",
    "    return np.exp(-days_age*DAYS_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "30 0.9417645335842487\n",
      "60 0.8869204367171575\n",
      "90 0.835270211411272\n",
      "120 0.7866278610665535\n",
      "150 0.7408182206817179\n",
      "180 0.697676326071031\n",
      "210 0.6570468198150567\n",
      "240 0.6187833918061408\n",
      "270 0.5827482523739896\n",
      "300 0.5488116360940264\n",
      "330 0.5168513344916992\n",
      "360 0.4867522559599717\n",
      "390 0.4584060113052235\n",
      "420 0.43171052342907973\n",
      "450 0.4065696597405991\n",
      "480 0.38289288597511206\n",
      "510 0.3605949401730783\n",
      "540 0.3395955256449391\n",
      "570 0.31981902181630384\n",
      "600 0.30119421191220214\n",
      "630 0.2836540264997704\n",
      "660 0.26713530196585034\n",
      "690 0.25157855305975646\n",
      "720 0.23692775868212176\n"
     ]
    }
   ],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "# Simulating 2 year of decay on relevance of a product \n",
    "for i in np.arange(0,365*2,30):    \n",
    "    print(i, prod_relevance_decay(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = None\n",
    "items_coocurrence_df = None\n",
    "items_coocurence_counts_df = None\n",
    "items_recent_pop_df = None\n",
    "items_temporal_relev_df = None\n",
    "\n",
    "def reset_item_logs_and_statistics():\n",
    "    global items_df, items_coocurrence_df, items_coocurence_counts_df, items_recent_pop_df, items_temporal_relev_df\n",
    "    \n",
    "    items_df = pd.DataFrame(columns={'pid': np.int64,\n",
    "                                 'csid': np.int32,\n",
    "                                 'ccid': np.int32,\n",
    "                                 'bid': np.int32,\n",
    "                                 'price': np.float,\n",
    "                                 'relative_price_to_avg_category': np.float,\n",
    "                                 'product_recency': np.float,\n",
    "                                 'first_ts': np.int,\n",
    "                                 'last_ts': np.int\n",
    "                                }).set_index('pid')\n",
    "    \n",
    "    items_coocurrence_df = pd.DataFrame(columns={'pid_a': np.int64, \n",
    "                                                 'pid_b': np.int64, \n",
    "                                                 'ts': np.int32, \n",
    "                                                 'count_flag': np.int16})\n",
    "    \n",
    "    items_coocurence_counts_df = None\n",
    "    items_recent_pop_df = None\n",
    "    items_temporal_relev_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid_b</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid_a</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pid_b  count\n",
       "pid_a               \n",
       "2           7   1106\n",
       "2           4   1091\n",
       "2          36    760\n",
       "2          13    624\n",
       "2           6    577\n",
       "...       ...    ...\n",
       "2        9663      1\n",
       "2        9684      1\n",
       "2        9692      1\n",
       "2         341      1\n",
       "2      193283      1\n",
       "\n",
       "[2354 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_coocurence_counts_df.loc[2].sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_df.index, min(n_samples, len(items_df)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_popularity_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_recent_pop_df.index, min(n_samples, len(items_recent_pop_df)), replace=False, \n",
    "                            p=items_recent_pop_df['prob']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coocurrence_sampling_item_ids(pid, n_samples):\n",
    "    samples = []\n",
    "    if pid in items_coocurence_counts_df.index:\n",
    "        coocurrent_df = items_coocurence_counts_df.loc[pid]\n",
    "        #Dealing with cases when there is only one co-occurrent item (loc() returns a Series)\n",
    "        if type(coocurrent_df) is pd.Series:\n",
    "            coocurrent_df = coocurrent_df.to_frame().T\n",
    "        coocurrent_df['probs'] = coocurrent_df['count'] / coocurrent_df['count'].sum()\n",
    "        samples = np.random.choice(coocurrent_df['pid_b'], min(n_samples, len(coocurrent_df)), replace=False, \n",
    "                                   p=coocurrent_df['probs']).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recency_sampling_item_ids(n_samples):\n",
    "    samples = np.random.choice(items_temporal_relev_df.index, min(n_samples, len(items_temporal_relev_df)), replace=False, \n",
    "                               p=items_temporal_relev_df.values).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_samples_item_ids(pid, n_samples, strategy, ignore_list=None):\n",
    "    #To ensure that after removing sessions from the current session we have the required number of samples\n",
    "    SAMPLES_MULITPLIER = 2\n",
    "    if strategy == UNIFORM_SAMPLING:\n",
    "        samples = get_uniform_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENCY_SAMPLING:\n",
    "        samples = get_recency_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENT_POPULARITY_SAMPLING:\n",
    "        samples = get_popularity_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)    \n",
    "    elif strategy == COOCURRENCE_SAMPLING:\n",
    "        samples = get_coocurrence_sampling_item_ids(pid, n_samples*SAMPLES_MULITPLIER)\n",
    "    \n",
    "        #Completing the list of samples based on global popularity\n",
    "        if len(samples) < n_samples:\n",
    "            samples += get_popularity_sampling_item_ids(n_samples - len(samples))\n",
    "    else:\n",
    "        raise Exception('Not a valid strategy. Should be: (uniform|recency|popularity|cooccurrence)')\n",
    "        \n",
    "    #Removing repeated entries\n",
    "    samples = list(set(samples))\n",
    "    #Removing samples from the ignore list\n",
    "    if ignore_list is not None:\n",
    "        samples = list([i for i in samples if i not in ignore_list])\n",
    "\n",
    "    return samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_candidate_samples_item_ids(63246, 10, strategy=COOCURRENCE_SAMPLING, ignore_list=[2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_item_ids(pids):\n",
    "    return items_df.loc[pids][['csid', 'ccid', 'bid', 'price', 'relative_price_to_avg_category', 'product_recency']] #\\\n",
    "    .to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "##%%time\n",
    "#l= np.random.choice(items_df.index, 50)\n",
    "#l= np.ones(50, dtype='int')*10\n",
    "#l = [10] * 50\n",
    "#l = [33284, 6, 21, 2588, 23069, 1570, 8230, 552, 50, 4152, 55864, 24636, 113213, 65601, 9283, 1097, 1104, 90, 2651, 607, 13920, 7785, 619, 8821, 9337, 4221, 8330, 7307, 8848, 19089, 112784, 2197, 15002, 3234, 3252, 1207, 2745, 40128, 30918, 4299, 61646, 23247, 213, 6871, 8921, 15582, 6367, 15077, 8935, 3820, 1266, 1790, 256, 258, 3844, 1808, 14109, 1311, 8481, 3362, 1318, 2348, 301, 29996, 19245, 1331, 3891, 10553, 830, 1855, 1867, 3415, 2933, 8571, 21371, 390, 903, 1425, 404, 5527, 1958, 23477, 6071, 59321, 7100, 4545, 1479, 28628, 10200, 16857, 3544, 479, 480, 6628, 35812, 2032, 3059, 28153, 2046]\n",
    "#for i in range(100):\n",
    "#    for j in range(10):\n",
    "#        x = get_features_for_item_ids(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padarray(A, size):\n",
    "    if len(A) > size:\n",
    "        A = A[:size]\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padarray([1,2,3], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_samples(session_pids, user_past_pids, n_samples, strategy):\n",
    "    neg_samples_dict = defaultdict(list)\n",
    "    \n",
    "    #Ignores session items and also recently interacted items\n",
    "    ignore_ids = set(np.hstack([session_pids, user_past_pids]))\n",
    "    \n",
    "    for pid in session_pids:\n",
    "        if pid != 0:\n",
    "            #Sampling item idds\n",
    "            neg_item_ids = get_candidate_samples_item_ids(pid, n_samples, \n",
    "                                                          ignore_list=ignore_ids,\n",
    "                                                         strategy=strategy\n",
    "                                                         )      \n",
    "            #Retrieving item features\n",
    "            neg_item_features_dict = get_features_for_item_ids(neg_item_ids)\n",
    "\n",
    "            \n",
    "            pids_padded = padarray(neg_item_ids, n_samples).astype(int)\n",
    "            neg_samples_dict['sess_neg_pids'].append(pids_padded)\n",
    "            \n",
    "            for k, v in neg_item_features_dict.items():\n",
    "                values = padarray(v, n_samples)\n",
    "                values = values.astype(int) if k in ['csid', 'ccid', 'bid'] else values.astype(float)\n",
    "                neg_samples_dict['sess_neg_{}'.format(k)].append(values)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            #Creating padding neg samples for each padding interactions\n",
    "            missing_padding_neg_samples = len(session_pids) - len(neg_samples_dict['sess_neg_pids'])                        \n",
    "            for k in neg_samples_dict:\n",
    "                neg_samples = neg_samples_dict[k]\n",
    "                neg_samples_zeros = np.zeros_like(neg_samples[0])\n",
    "                for p in range(missing_padding_neg_samples):\n",
    "                    #Copying shape and dtype from the neg samples of the first interaction\n",
    "                    neg_samples.append(neg_samples_zeros)\n",
    "         \n",
    "    #Concatenating neg. samples of all session interactions because Petastorm data loader \n",
    "    #does not support lists of lists. It will require reshaping neg. samples features inside the Pytorch model\n",
    "    for k in neg_samples_dict:  \n",
    "        neg_samples_dict[k] = np.hstack(neg_samples_dict[k])        \n",
    "\n",
    "    return neg_samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_neg_samples([10,20,30, 0, 0], n_samples=2, strategy='popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_rows_to_parquet(new_rows_df, path):\n",
    "    global pq_writer\n",
    "    new_rows_pa = pyarrow.Table.from_pandas(new_rows_df)\n",
    "    if pq_writer is None:\n",
    "        #Creating parent folder recursively\n",
    "        parent_folder = os.path.dirname(os.path.abspath(path))\n",
    "        if not os.path.exists(parent_folder):\n",
    "            os.makedirs(parent_folder)\n",
    "        #Creating parquet file\n",
    "        pq_writer = pq.ParquetWriter(path, new_rows_pa.schema) \n",
    "    pq_writer.write_table(new_rows_pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates neg. samples for all sessions and creates new parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "[Day 0] Loading sessions from parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches\n",
      "batch_id 0\n",
      "[Batch 0] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:09,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 1\n",
      "[Batch 1] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:39, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 2\n",
      "[Batch 2] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:08, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 3\n",
      "[Batch 3] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:34, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 4\n",
      "[Batch 4] Updating item stats\n",
      "[Batch 4] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [02:01, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 5\n",
      "[Batch 5] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [02:26, 23.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [02:50, 23.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [03:15, 24.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 8\n",
      "[Batch 8] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [03:44, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 9\n",
      "[Batch 9] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [04:09, 25.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [04:33, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 11\n",
      "[Batch 11] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [05:01, 26.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [05:25, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [05:48, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 14\n",
      "[Batch 14] Updating item stats\n",
      "[Batch 14] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [06:22, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [06:42, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [07:06, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 17\n",
      "[Batch 17] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [07:38, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [08:03, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 19\n",
      "[Batch 19] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [08:28, 26.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 20\n",
      "[Batch 20] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [08:56, 26.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [09:20, 25.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [09:42, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 23\n",
      "[Batch 23] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [10:10, 25.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 24\n",
      "[Batch 24] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [10:35, 25.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [10:59, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 26\n",
      "[Batch 26] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [11:29, 26.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [11:54, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [12:20, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 29\n",
      "[Batch 29] Updating item stats\n",
      "[Batch 29] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [12:47, 26.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [13:13, 26.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [13:35, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 32\n",
      "[Batch 32] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [14:00, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [14:21, 23.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 34\n",
      "[Batch 34] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [14:46, 23.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 35\n",
      "[Batch 35] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [15:11, 24.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [15:32, 23.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [16:09, 27.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 38\n",
      "[Batch 38] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [16:46, 30.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 39\n",
      "[Batch 39] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [17:10, 28.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [17:36, 27.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 41\n",
      "[Batch 41] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [18:10, 29.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [18:40, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [19:10, 29.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 44\n",
      "[Batch 44] Updating item stats\n",
      "[Batch 44] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [19:38, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [20:00, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [20:21, 25.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 47\n",
      "[Batch 47] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [20:47, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [21:09, 24.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 49\n",
      "[Batch 49] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [21:37, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 50\n",
      "[Batch 50] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "51it [22:15, 29.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "52it [22:42, 28.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "53it [23:19, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 53\n",
      "[Batch 53] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [23:55, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 54\n",
      "[Batch 54] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [24:40, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "56it [25:22, 38.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 56\n",
      "[Batch 56] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "57it [26:20, 43.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "58it [27:21, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "59it [28:16, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 59\n",
      "[Batch 59] Updating item stats\n",
      "[Batch 59] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "60it [29:24, 56.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [29:53, 29.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8c6624011a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#For each row (session)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0minsert_update_session_items_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mappend_session_coocurrences_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-cf66bf355649>\u001b[0m in \u001b[0;36minsert_update_session_items_metadata\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     34\u001b[0m                                            \u001b[0;34m'product_recency'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprod_recency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            \u001b[0;34m'first_ts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfirst_ts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                                            'last_ts': last_ts})\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# TODO: passing np.float64 to not break anything yet. See GH-17261\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         s = create_series_with_explicit_dtype(\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mcreate_series_with_explicit_dtype\u001b[0;34m(data, index, dtype, name, copy, fastpath, dtype_if_empty)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     return Series(\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   5356\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5358\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cast_data_without_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                     return cls(\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_maybe_cast_data_without_dtype\u001b[0;34m(subarr)\u001b[0m\n\u001b[1;32m   5484\u001b[0m     )\n\u001b[1;32m   5485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5486\u001b[0;31m     \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"integer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib._try_infer_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;31m# provides dtype.name.__get__, documented as returning a \"bit name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pq_writer = None\n",
    "\n",
    "reset_item_logs_and_statistics()\n",
    "try:\n",
    "    #For each file (day)\n",
    "    for idx_day, input_file in enumerate(input_parquet_files):\n",
    "        print('='*40)\n",
    "        print('[Day {}] Loading sessions from parquet: {}'.format(idx_day, input_file))\n",
    "        output_filename = get_output_path_parquet_neg_samples(input_file)\n",
    "        \n",
    "        if os.path.exists(output_filename):\n",
    "            raise Exception('Output parquet file already exists')\n",
    "        \n",
    "        #Loading parquet file and sorting sessions by timestamp\n",
    "        sessions_df = pd.read_parquet(input_file)\n",
    "        sessions_df.sort_values('session_start_ts', inplace=True)\n",
    "                \n",
    "        new_rows = []\n",
    "        \n",
    "        print('Processing batches')\n",
    "        #For each batch\n",
    "        for batch_id, batch in tqdm(enumerate(split_dataframe_into_chuncks_generator(sessions_df, \n",
    "                                                                                chunk_size = BATCH_SIZE))):\n",
    "            print('batch_id', batch_id)            \n",
    "            #For each row (session)\n",
    "            for i, row in batch.iterrows():\n",
    "                insert_update_session_items_metadata(row)\n",
    "                append_session_coocurrences_log(row)\n",
    "                \n",
    "                \n",
    "                #Ignoring first batch (not computing neg. samples nor saving to parquet)\n",
    "                if batch_id > 0:   \n",
    "                    #Generating neg. samples for each interaction in the session\n",
    "                    session_neg_samples_by_pid_dict = generate_neg_samples(row['sess_pid_seq'], \n",
    "                                                                           row['user_pid_seq_bef_sess'],\n",
    "                                                                           NUM_NEG_SAMPLES, \n",
    "                                                                           strategy=NEGATIVE_SAMPLING_STRATEGY\n",
    "                                                                          )\n",
    "                    #Merging user and session features with neg samples for the session\n",
    "                    new_row_with_neg_samples_dict = {**row.to_dict(), **session_neg_samples_by_pid_dict}\n",
    "                    new_rows.append(new_row_with_neg_samples_dict)\n",
    "                    \n",
    "            \n",
    "            #Each N batches updates item statistics (popularity, recency, co-occurrence)\n",
    "            #Ps. Do the update for all the first five batches of the first file , for better sampling\n",
    "            if (batch_id % BATCHES_TO_UPDATE_ITEM_STATS == BATCHES_TO_UPDATE_ITEM_STATS-1) or \\\n",
    "               (idx_day == 0 and batch_id < 5):\n",
    "                print('[Batch {}] Updating item stats'.format(batch_id))\n",
    "                remove_old_interactions(ITEM_STATS_KEEP_LAST_N_DAYS)\n",
    "                if NEGATIVE_SAMPLING_STRATEGY in [RECENT_POPULARITY_SAMPLING, COOCURRENCE_SAMPLING]:\n",
    "                    concat_sessions_coocurrences_log()\n",
    "                    update_items_coocurrences_counts()\n",
    "                    if RECENT_POPULARITY_SAMPLING:\n",
    "                        update_items_recent_popularity()\n",
    "                if NEGATIVE_SAMPLING_STRATEGY == RECENCY_SAMPLING:\n",
    "                    update_items_temporal_relevance_decay()\n",
    "                \n",
    "            #Each N batches appends the new rows with neg. samples to parquet file\n",
    "            if batch_id % BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES == BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES-1: \n",
    "                print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "                append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "                del(new_rows)\n",
    "                new_rows = []\n",
    "            \n",
    "               \n",
    "        #Save pending rows\n",
    "        if len(new_rows) > 0:\n",
    "            print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "            append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "            del(new_rows)\n",
    "            new_rows = []\n",
    "            \n",
    "        #Flushing and releasing the current parquet file and proceeding for the new date\n",
    "        pq_writer.close()\n",
    "        pq_writer = None\n",
    "                \n",
    "        del(sessions_df)\n",
    "        gc.collect()\n",
    "finally:\n",
    "    if pq_writer:\n",
    "        pq_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the parquet with Negative samples with Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.pytorch import DataLoader\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.unischema import UnischemaField\n",
    "from petastorm.unischema import Unischema\n",
    "from petastorm.codecs import NdarrayCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_neg_parquet_path = 'file:///home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01_full.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_schema_full = [\n",
    "  UnischemaField('user_idx', np.int, (), None, True),\n",
    "#   UnischemaField('user_session', str_, (), None, True),\n",
    "  UnischemaField('sess_seq_len', np.int, (), None, False),\n",
    "  UnischemaField('session_start_ts', np.int64, (), None, True),\n",
    "  UnischemaField('user_seq_length_bef_sess', np.int, (), None, False),\n",
    "  UnischemaField('user_elapsed_days_bef_sess', np.float, (), None, True),\n",
    "  UnischemaField('user_elapsed_days_log_bef_sess_norm', np.double, (), None, True),\n",
    "  UnischemaField('sess_pid_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etime_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etype_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_csid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_ccid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_bid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_price_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_dtime_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_product_recency_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_relative_price_to_avg_category_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('user_pid_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etime_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etype_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_csid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_ccid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_bid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_price_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_dtime_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_product_recency_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_relative_price_to_avg_category_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_pids', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_csid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_ccid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_bid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_price', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_relative_price_to_avg_category', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_product_recency', np.float, (None,), None, True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/anaconda3/envs/transf4rec/lib/python3.7/site-packages/petastorm/unischema.py:197: UserWarning: Can not create dynamic property user_et_dayofmonth_cos_seq_bef_sess because it conflicts with an existing property of Unischema\n",
      "  'Unischema').format(f.name))\n",
      "/home/gmoreira/anaconda3/envs/transf4rec/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "  column_as_pandas = column.data.chunks[0].to_pandas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0\n",
      "{'user_idx': tensor([550479254, 547827437]), 'sess_seq_len': tensor([21,  3]), 'session_start_ts': tensor([1569921441, 1569921441]), 'user_seq_length_bef_sess': tensor([0, 0]), 'user_elapsed_days_bef_sess': tensor([nan, nan], dtype=torch.float64), 'user_elapsed_days_log_bef_sess_norm': tensor([nan, nan], dtype=torch.float64), 'sess_pid_seq': tensor([[125685,   5713,   3066,  11869,  10670,   4312,  19814,  63599,   8931,\n",
      "           6909,   7571,   3390,   1697,   3616,  21046,   2666,   7386,   3976,\n",
      "          23679,  28513],\n",
      "        [   515,     57,    331,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]]), 'sess_etime_seq': tensor([[1569921441, 1569921527, 1569921549, 1569921628, 1569921649, 1569921670,\n",
      "         1569921712, 1569921752, 1569921793, 1569921955, 1569922051, 1569922061,\n",
      "         1569922096, 1569922146, 1569922168, 1569922273, 1569922283, 1569922301,\n",
      "         1569922345, 1569922393],\n",
      "        [1569921441, 1569921463, 1569921471,          0,          0,          0,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0,          0,          0,          0,          0,\n",
      "                  0,          0]]), 'sess_etype_seq': tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'sess_csid_seq': tensor([[362, 159, 159, 159, 159, 159, 159, 159, 159, 159,  88, 205, 159,  88,\n",
      "          88,  28,  28,  28,  28,  28],\n",
      "        [  8,   8,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]], dtype=torch.int32), 'sess_ccid_seq': tensor([[131, 131, 131, 131, 131, 131, 131, 131, 131, 131,  14, 131, 131,  14,\n",
      "          14,  18,  18,  18,  18,  18],\n",
      "        [  8,   8,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]], dtype=torch.int32), 'sess_bid_seq': tensor([[4305, 4305,   32, 4305, 4305, 4305, 4305, 4305, 4305, 4305, 4305,  282,\n",
      "           32, 4305, 4305, 4305, 4305,  132, 4305, 4305],\n",
      "        [   2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int32), 'sess_price_seq': tensor([[-2.5044,  0.8894,  0.7539,  1.2824,  0.8940,  0.8894,  1.6242,  0.2981,\n",
      "          2.0262,  2.1064, -0.1115, -1.0654,  0.5714, -0.0960, -0.0823,  1.4385,\n",
      "          0.9771,  0.6699,  1.0600,  1.0333],\n",
      "        [-0.4354, -0.7092, -0.7061,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_dtime_seq': tensor([[-0.0230, -0.0144, -0.0208, -0.0151, -0.0209, -0.0209, -0.0188, -0.0198,\n",
      "         -0.0189, -0.0197, -0.0134, -0.0220, -0.0195, -0.0204, -0.0208, -0.0125,\n",
      "         -0.0220, -0.0212, -0.0186, -0.0182],\n",
      "        [-0.0230, -0.0208, -0.0222,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_product_recency_seq': tensor([[-3.5630, -3.5637, -3.5722, -3.5630, -3.5702, -3.5588, -3.5685, -3.7386,\n",
      "         -3.5727, -3.5590, -3.4702, -3.8292, -3.5529, -3.5856, -3.6987, -3.5762,\n",
      "         -3.5526, -3.5521, -3.5541, -3.5564],\n",
      "        [-3.5637, -3.5573, -3.5698,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_relative_price_to_avg_category_seq': tensor([[-0.6317, -0.1639, -0.2915,  0.3509, -0.1591, -0.1639,  1.0500, -0.5942,\n",
      "          2.3469,  2.6908, -0.1142,  0.0661, -0.4331, -0.0972, -0.0818,  0.4311,\n",
      "         -0.1852, -0.4401, -0.0983, -0.1273],\n",
      "        [-0.5030, -0.6456, -0.6442,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_hour_sin_seq': tensor([[0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071,\n",
      "         0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071, 0.7071,\n",
      "         0.7071, 0.7071],\n",
      "        [0.7071, 0.7071, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_hour_cos_seq': tensor([[-0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071,\n",
      "         -0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071, -0.7071,\n",
      "         -0.7071, -0.7071, -0.7071, -0.7071],\n",
      "        [-0.7071, -0.7071, -0.7071,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_month_sin_seq': tensor([[-0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660,\n",
      "         -0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660, -0.8660,\n",
      "         -0.8660, -0.8660, -0.8660, -0.8660],\n",
      "        [-0.8660, -0.8660, -0.8660,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_month_cos_seq': tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofweek_sin_seq': tensor([[0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339,\n",
      "         0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339, 0.4339,\n",
      "         0.4339, 0.4339],\n",
      "        [0.4339, 0.4339, 0.4339, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofweek_cos_seq': tensor([[-0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010,\n",
      "         -0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010, -0.9010,\n",
      "         -0.9010, -0.9010, -0.9010, -0.9010],\n",
      "        [-0.9010, -0.9010, -0.9010,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]]), 'sess_et_dayofmonth_sin_seq': tensor([[0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013,\n",
      "         0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013, 0.2013,\n",
      "         0.2013, 0.2013],\n",
      "        [0.2013, 0.2013, 0.2013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'sess_et_dayofmonth_cos_seq': tensor([[0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795,\n",
      "         0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795, 0.9795,\n",
      "         0.9795, 0.9795],\n",
      "        [0.9795, 0.9795, 0.9795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]]), 'user_pid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'user_etime_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'user_etype_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_csid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_ccid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_bid_seq_bef_sess': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'user_price_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_dtime_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_product_recency_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_relative_price_to_avg_category_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_hour_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_hour_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_month_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_month_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofweek_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofweek_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofmonth_sin_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'user_et_dayofmonth_cos_seq_bef_sess': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'sess_neg_pids': tensor([[173824,    131,      6,  ...,  26878,      0,      0],\n",
      "        [ 36353,   1031,    524,  ...,      0,      0,      0]]), 'sess_neg_csid': tensor([[601,  10,   2,  ...,  28,   0,   0],\n",
      "        [  8,   8,   8,  ...,   0,   0,   0]]), 'sess_neg_ccid': tensor([[103,   3,   2,  ...,  18,   0,   0],\n",
      "        [  8,   8,   8,  ...,   0,   0,   0]]), 'sess_neg_bid': tensor([[ 793,    4,    2,  ..., 4305,    0,    0],\n",
      "        [ 145,    2,   47,  ...,    0,    0,    0]]), 'sess_neg_price': tensor([[-0.9275, -0.6612,  0.5109,  ...,  1.6356,  0.0000,  0.0000],\n",
      "        [-0.4936, -0.3250, -0.1989,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64), 'sess_neg_relative_price_to_avg_category': tensor([[ 0.2254, -0.7917, -0.4009,  ...,  0.8199,  0.0000,  0.0000],\n",
      "        [-0.5374, -0.4307, -0.3352,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64), 'sess_neg_product_recency': tensor([[-3.8576, -3.4990, -3.4690,  ..., -3.8576,  0.0000,  0.0000],\n",
      "        [-3.7296, -3.5652, -3.5564,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64)}\n",
      "torch.Size([2, 1000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-da8077b5c1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sess_neg_product_recency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/petastorm/pytorch.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/petastorm/reader.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m\"\"\"Joins all worker threads/processes. Will block until all worker workers have been fully terminated.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misAlive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profiling_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transf4rec/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with DataLoader(\n",
    "    make_batch_reader(input_with_neg_parquet_path, \n",
    "                num_epochs=1,\n",
    "                # transform_spec=transform\n",
    "                schema_fields=recsys_schema_full,\n",
    "    ), batch_size=2) as train_loader:\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(\"i:{}\".format(i))\n",
    "        print(batch)\n",
    "        print(batch['sess_neg_product_recency'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1652, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    }
   ],
   "source": [
    "items_df = pd.DataFrame(columns=[{'pid': np.int32, 'c1': np.int64, 'n1': np.float32}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_empty(columns, dtypes, index=None):\n",
    "    assert len(columns)==len(dtypes)\n",
    "    df = pd.DataFrame(index=index)\n",
    "    for c,d in zip(columns, dtypes):\n",
    "        df[c] = pd.Series(dtype=d)\n",
    "    return df\n",
    "\n",
    "items_df = df_empty(columns=['pid', 'c1', 'n1'], dtypes=[np.int32, np.int64, np.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_empty(column_dtype_mapping, index=None):    \n",
    "    df = pd.DataFrame()\n",
    "    for c in column_dtype_mapping:\n",
    "        df[c] = pd.Series(dtype=column_dtype_mapping[c])\n",
    "    return df.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = df_empty({'pid': np.int32, 'c1': np.int64, 'n1': np.float32}, index=['pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "items2_df = pd.DataFrame([(1, 10, 12.5)], columns={'pid': np.int32, 'c1': np.int64, 'n1': np.float32}).set_index('pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "items2_df.loc[lambda df: df['c1'] == 10] = [20, 20.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1      int64\n",
       "n1    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items2_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1    10.0\n",
       "n1    12.5\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items2_df.loc[1, ['c1', 'n1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "items2_df.loc[2] = [20, 15.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items2_df.loc[2] = items2_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c1    n1\n",
       "pid          \n",
       "1    10  12.5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       c1    n1\n",
       "pid            \n",
       "1    10.0  12.5\n",
       "2    20.0  15.7"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.loc[0] = pd.Series({'c1': 10, 'n1': 50.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       c1    n1\n",
       "pid            \n",
       "0    10.0  50.5"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.Pandas'>\n"
     ]
    }
   ],
   "source": [
    "for i in items2_df.itertuples():\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCDataFrame',\n",
       " 'ABCIndexClass',\n",
       " 'ABCMultiIndex',\n",
       " 'ABCSeries',\n",
       " 'Any',\n",
       " 'Appender',\n",
       " 'Axes',\n",
       " 'Axis',\n",
       " 'BlockManager',\n",
       " 'CachedAccessor',\n",
       " 'Categorical',\n",
       " 'DataFrame',\n",
       " 'DatetimeIndex',\n",
       " 'DatetimeLikeArray',\n",
       " 'Dtype',\n",
       " 'ExtensionArray',\n",
       " 'FilePathOrBuffer',\n",
       " 'FrozenSet',\n",
       " 'Hashable',\n",
       " 'IO',\n",
       " 'Index',\n",
       " 'Iterable',\n",
       " 'Level',\n",
       " 'List',\n",
       " 'NDFrame',\n",
       " 'Optional',\n",
       " 'PY37',\n",
       " 'PeriodIndex',\n",
       " 'Renamer',\n",
       " 'Sequence',\n",
       " 'Series',\n",
       " 'Set',\n",
       " 'SparseFrameAccessor',\n",
       " 'StringIO',\n",
       " 'Substitution',\n",
       " 'TYPE_CHECKING',\n",
       " 'Tuple',\n",
       " 'Type',\n",
       " 'Union',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_from_nested_dict',\n",
       " '_merge_doc',\n",
       " '_numeric_only_doc',\n",
       " '_put_str',\n",
       " '_shared_doc_kwargs',\n",
       " '_shared_docs',\n",
       " 'abc',\n",
       " 'algorithms',\n",
       " 'arrays_to_mgr',\n",
       " 'cast',\n",
       " 'cast_scalar_to_array',\n",
       " 'check_bool_indexer',\n",
       " 'coerce_to_dtypes',\n",
       " 'collections',\n",
       " 'com',\n",
       " 'console',\n",
       " 'convert_to_index_sliceable',\n",
       " 'dedent',\n",
       " 'deprecate_kwarg',\n",
       " 'dispatch_fill_zeros',\n",
       " 'ensure_float64',\n",
       " 'ensure_index',\n",
       " 'ensure_index_from_sequences',\n",
       " 'ensure_int64',\n",
       " 'ensure_platform_int',\n",
       " 'find_common_type',\n",
       " 'fmt',\n",
       " 'get_filepath_or_buffer',\n",
       " 'get_names_from_index',\n",
       " 'get_option',\n",
       " 'groupby_generic',\n",
       " 'ibase',\n",
       " 'import_optional_dependency',\n",
       " 'infer_dtype_from_object',\n",
       " 'infer_dtype_from_scalar',\n",
       " 'init_dict',\n",
       " 'init_ndarray',\n",
       " 'invalidate_string_dtypes',\n",
       " 'is_bool_dtype',\n",
       " 'is_dict_like',\n",
       " 'is_dtype_equal',\n",
       " 'is_extension_array_dtype',\n",
       " 'is_float_dtype',\n",
       " 'is_hashable',\n",
       " 'is_integer',\n",
       " 'is_integer_dtype',\n",
       " 'is_iterator',\n",
       " 'is_list_like',\n",
       " 'is_named_tuple',\n",
       " 'is_object_dtype',\n",
       " 'is_scalar',\n",
       " 'is_sequence',\n",
       " 'isna',\n",
       " 'itertools',\n",
       " 'lib',\n",
       " 'libalgos',\n",
       " 'ma',\n",
       " 'masked_rec_array_to_mgr',\n",
       " 'maybe_cast_to_datetime',\n",
       " 'maybe_convert_platform',\n",
       " 'maybe_downcast_to_dtype',\n",
       " 'maybe_droplevels',\n",
       " 'maybe_infer_to_datetimelike',\n",
       " 'maybe_upcast',\n",
       " 'maybe_upcast_putmask',\n",
       " 'nanops',\n",
       " 'needs_i8_conversion',\n",
       " 'notna',\n",
       " 'np',\n",
       " 'nv',\n",
       " 'ops',\n",
       " 'pandas',\n",
       " 'pprint_thing',\n",
       " 'reorder_arrays',\n",
       " 'rewrite_axis_style_signature',\n",
       " 'sanitize_index',\n",
       " 'sys',\n",
       " 'to_arrays',\n",
       " 'validate_axis_style_args',\n",
       " 'validate_bool_kwarg',\n",
       " 'validate_percentile',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pd.core.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(items_df[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series({\n",
    "                                'sess_csid_seq': 100,\n",
    "                                'sess_price_seq': 270.90,\n",
    "                                'first_ts': 1594100000, \n",
    "                                'last_ts':  1594100000,\n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_csid_seq     1.000000e+02\n",
       "sess_price_seq    2.709000e+02\n",
       "first_ts          1.594100e+09\n",
       "last_ts           1.594100e+09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [c1, n1]\n",
       "Index: []"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-2832e50c83c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                 \u001b[0;34m'sess_price_seq'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m270.90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0;34m'first_ts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1594100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                 \u001b[0;34m'last_ts'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;36m1594100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                              }\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0;31m# must have conforming columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "items_df.loc[0] = {'sess_csid_seq': 100,\n",
    "                    'sess_price_seq': 270.90,\n",
    "                    'first_ts': 1594100000, \n",
    "                    'last_ts':  1594100000,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>60.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c1    n1\n",
       "pid          \n",
       "1    12  60.9"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>n1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>60.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0    12\n",
       "dtype: int64</td>\n",
       "      <td>0    60.9\n",
       "dtype: float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       c1                        n1\n",
       "pid                                                \n",
       "1                      12                      60.9\n",
       "0    0    12\n",
       "dtype: int64  0    60.9\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1    object\n",
       "n1    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
