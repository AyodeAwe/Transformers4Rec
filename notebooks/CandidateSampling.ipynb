{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PARQUET_PATH = \"/home/gmoreira/dataset/ecommerce_preproc_2019-*/ecommerce_preproc.parquet/session_start_date=*\"\n",
    "OUTPUT_NEG_SAMPLES_PARQUET_PATH = \"/home/gmoreira/dataset/neg_samples.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIFORM_SAMPLING = 'uniform'\n",
    "RECENCY_SAMPLING = 'recency'\n",
    "RECENT_POPULARITY_SAMPLING = 'popularity'\n",
    "COOCURRENCE_SAMPLING = 'cooccurrence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_SAMPLING_STRATEGY = COOCURRENCE_SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "BATCHES_TO_UPDATE_ITEM_STATS = 3\n",
    "BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES = 5\n",
    "ITEM_STATS_KEEP_LAST_N_DAYS = 1.0\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_NEG_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-30',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-31',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-01',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-02',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-03',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-04',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-05',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-06',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-07',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-08',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-09',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-10',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-11',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-12',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-13',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-14',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-15',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-16',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-17',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-18',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-19',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-20',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-21',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-22',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-23',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-24',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-25',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-26',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-27',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-28',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-29',\n",
       " '/home/gmoreira/dataset/ecommerce_preproc_2019-11/ecommerce_preproc.parquet/session_start_date=2019-11-30']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_parquet_files = sorted(glob.glob(INPUT_PARQUET_PATH+'*'))\n",
    "input_parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path_parquet_neg_samples(input_parquet_filename):\n",
    "    return input_parquet_filename \\\n",
    "        .replace('ecommerce_preproc.parquet', 'ecommerce_preproc_neg_samples_{}_strategy_{}.parquet' \\\n",
    "                     .format(NUM_NEG_SAMPLES, NEGATIVE_SAMPLING_STRATEGY)) + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_files(data_paths):\n",
    "    paths = [['file://' + p for p in glob.glob(path + \"/*.parquet\")] for path in data_paths]\n",
    "    return list(itertools.chain.from_iterable(paths))\n",
    "input_parquet_files = get_files([INPUT_PARQUET_PATH])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Works but cannot be used now because the preprocessed sessions are not sorted by timestamp\n",
    "def read_parquet_generator(filenames, batch_size=128):\n",
    "    for filename in filenames:\n",
    "        for batch in pq.read_table(filename).to_batches(batch_size):\n",
    "            yield batch.to_pandas()\n",
    "            \n",
    "parquet_reader = read_parquet_generator([INPUT_PARQUET_PATH], batch_size=BATCH_SIZE)            \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_into_chuncks_generator(df, chunk_size): \n",
    "    number_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(number_chunks):\n",
    "        yield df[i*chunk_size:(i+1)*chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_update_session_items_metadata(row):\n",
    "    #Uses the session start as the event timestamp (as the sess_etime_seq might sometimes be many days before because of outlier sessions with more than 120 min duration (< 1%))\n",
    "    etime = row['session_start_ts']\n",
    "    #For each session\n",
    "    for pid, csid, ccid, bid, price, relative_price, prod_recency in zip(\n",
    "                                                        #row['sess_etime_seq'],\n",
    "                                                        row['sess_pid_seq'], \n",
    "                                                        row['sess_csid_seq'],\n",
    "                                                        row['sess_ccid_seq'],\n",
    "                                                        row['sess_bid_seq'],\n",
    "                                                        row['sess_price_seq'],\n",
    "                                                        row['sess_relative_price_to_avg_category_seq'],\n",
    "                                                        row['sess_product_recency_seq']):\n",
    "\n",
    "        #If this item was not processed before\n",
    "        if pid != 0:\n",
    "            if pid in items_df.index:\n",
    "                curr_row = items_df.loc[pid]\n",
    "\n",
    "                first_ts = curr_row['first_ts']\n",
    "                last_ts = curr_row['last_ts']\n",
    "                if etime > last_ts:\n",
    "                    last_ts = etime\n",
    "            else:\n",
    "                first_ts = etime\n",
    "                last_ts = etime\n",
    "\n",
    "            #Including or updating the item metadata\n",
    "            items_df.loc[pid] = pd.Series({'csid': csid,\n",
    "                                           'ccid': ccid,\n",
    "                                           'bid': bid,\n",
    "                                           'price': price,\n",
    "                                           'relative_price_to_avg_category': relative_price,\n",
    "                                           'product_recency': prod_recency,\n",
    "                                           'first_ts': first_ts,\n",
    "                                           'last_ts': last_ts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_cooccurrences_log_list = []\n",
    "\n",
    "def append_session_coocurrences_log(row):\n",
    "    global session_cooccurrences_log_list\n",
    "    min_ts = min([t for t in row['sess_etime_seq'] if t != 0])\n",
    "    valid_pids = list(set(list([p for p in row['sess_pid_seq'] if p != 0])))\n",
    "    \n",
    "    if len(valid_pids) > 1:\n",
    "        items_permutations = permutations(valid_pids, 2)        \n",
    "        new_coo_df = pd.DataFrame(items_permutations, columns=['pid_a', 'pid_b'])\n",
    "        new_coo_df['ts'] = min_ts\n",
    "        #This flag is used for counting unique values from this table to compute popularity\n",
    "        new_coo_df['count_flag'] = ([1] + [0]*(len(valid_pids)-2))*len(valid_pids)\n",
    "        \n",
    "        session_cooccurrences_log_list.append(new_coo_df)\n",
    "        #items_coocurrence_df = pd.concat([items_coocurrence_df, new_coo_df])\n",
    "        #items_coocurrence_df.append(new_coo_df, ignore_index=True)\n",
    "\n",
    "def concat_sessions_coocurrences_log():\n",
    "    global items_coocurrence_df, session_cooccurrences_log_list\n",
    "    items_coocurrence_df = pd.concat([items_coocurrence_df] + session_cooccurrences_log_list)\n",
    "    session_cooccurrences_log_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_old_interactions(keep_last_n_days):\n",
    "    global items_coocurrence_df\n",
    "    last_ts = items_coocurrence_df['ts'].max()\n",
    "    keep_last_n_secs = keep_last_n_days * 24 * 60 * 60\n",
    "    items_coocurrence_df = items_coocurrence_df[items_coocurrence_df['ts'] >= (last_ts - keep_last_n_secs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_temporal_relevance_decay():    \n",
    "    global items_temporal_relev_df\n",
    "    max_reference_ts = items_df['first_ts'].max()\n",
    "    prods_days_age = (max_reference_ts - items_df['first_ts']) / (60 * 60 * 24)\n",
    "\n",
    "    time_relev_by_item_series = prod_relevance_decay(prods_days_age)\n",
    "    items_temporal_relev_df = time_relev_by_item_series / time_relev_by_item_series.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_coocurrences_counts():\n",
    "    global items_coocurence_counts_df\n",
    "    items_coocurence_counts_df = items_coocurrence_df.groupby(['pid_a','pid_b']).size().to_frame('count') \\\n",
    "                                    .reset_index(level=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_items_recent_popularity():\n",
    "    global items_recent_pop_df\n",
    "    items_recent_pop_df = items_coocurrence_df[items_coocurrence_df['count_flag'] == True] \\\n",
    "            .groupby(['pid_a']).size().to_frame('count')\n",
    "    items_recent_pop_df['prob'] = items_recent_pop_df['count'] / items_recent_pop_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "\n",
    "def prod_relevance_decay(days_age):\n",
    "    return np.exp(-days_age*DAYS_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "30 0.9417645335842487\n",
      "60 0.8869204367171575\n",
      "90 0.835270211411272\n",
      "120 0.7866278610665535\n",
      "150 0.7408182206817179\n",
      "180 0.697676326071031\n",
      "210 0.6570468198150567\n",
      "240 0.6187833918061408\n",
      "270 0.5827482523739896\n",
      "300 0.5488116360940264\n",
      "330 0.5168513344916992\n",
      "360 0.4867522559599717\n",
      "390 0.4584060113052235\n",
      "420 0.43171052342907973\n",
      "450 0.4065696597405991\n",
      "480 0.38289288597511206\n",
      "510 0.3605949401730783\n",
      "540 0.3395955256449391\n",
      "570 0.31981902181630384\n",
      "600 0.30119421191220214\n",
      "630 0.2836540264997704\n",
      "660 0.26713530196585034\n",
      "690 0.25157855305975646\n",
      "720 0.23692775868212176\n"
     ]
    }
   ],
   "source": [
    "# (83% of relevance in one quarter, 70% in one semester, 50% in one year and 23% in two years)\n",
    "DAYS_DECAY_FACTOR = 0.002\n",
    "# Simulating 2 year of decay on relevance of a product \n",
    "for i in np.arange(0,365*2,30):    \n",
    "    print(i, prod_relevance_decay(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = None\n",
    "items_coocurrence_df = None\n",
    "items_coocurence_counts_df = None\n",
    "items_recent_pop_df = None\n",
    "items_temporal_relev_df = None\n",
    "\n",
    "def reset_item_logs_and_statistics():\n",
    "    global items_df, items_coocurrence_df, items_coocurence_counts_df, items_recent_pop_df, items_temporal_relev_df\n",
    "    \n",
    "    items_df = pd.DataFrame(columns={'pid': np.int64,\n",
    "                                 'csid': np.int32,\n",
    "                                 'ccid': np.int32,\n",
    "                                 'bid': np.int32,\n",
    "                                 'price': np.float,\n",
    "                                 'relative_price_to_avg_category': np.float,\n",
    "                                 'product_recency': np.float,\n",
    "                                 'first_ts': np.int,\n",
    "                                 'last_ts': np.int\n",
    "                                }).set_index('pid')\n",
    "    \n",
    "    items_coocurrence_df = pd.DataFrame(columns={'pid_a': np.int64, \n",
    "                                                 'pid_b': np.int64, \n",
    "                                                 'ts': np.int32, \n",
    "                                                 'count_flag': np.int16})\n",
    "    \n",
    "    items_coocurence_counts_df = None\n",
    "    items_recent_pop_df = None\n",
    "    items_temporal_relev_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_df.index, min(n_samples, len(items_df)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_popularity_sampling_item_ids(n_samples):\n",
    "    return np.random.choice(items_recent_pop_df.index, min(n_samples, len(items_recent_pop_df)), replace=False, \n",
    "                            p=items_recent_pop_df['prob']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coocurrence_sampling_item_ids(pid, n_samples):\n",
    "    samples = []\n",
    "    if pid in items_coocurence_counts_df.index:\n",
    "        coocurrent_df = items_coocurence_counts_df.loc[pid]\n",
    "        #Dealing with cases when there is only one co-occurrent item (loc() returns a Series)\n",
    "        if type(coocurrent_df) is pd.Series:\n",
    "            coocurrent_df = coocurrent_df.to_frame().T\n",
    "        coocurrent_df['probs'] = coocurrent_df['count'] / coocurrent_df['count'].sum()\n",
    "        samples = np.random.choice(coocurrent_df['pid_b'], min(n_samples, len(coocurrent_df)), replace=False, \n",
    "                                   p=coocurrent_df['probs']).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recency_sampling_item_ids(n_samples):\n",
    "    samples = np.random.choice(items_temporal_relev_df.index, min(n_samples, len(items_temporal_relev_df)), replace=False, \n",
    "                               p=items_temporal_relev_df.values).tolist()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_samples_item_ids(pid, n_samples, strategy, ignore_list=None):\n",
    "    #To ensure that after removing sessions from the current session we have the required number of samples\n",
    "    SAMPLES_MULITPLIER = 2\n",
    "    if strategy == UNIFORM_SAMPLING:\n",
    "        samples = get_uniform_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENCY_SAMPLING:\n",
    "        samples = get_recency_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)\n",
    "    elif strategy == RECENT_POPULARITY_SAMPLING:\n",
    "        samples = get_popularity_sampling_item_ids(n_samples*SAMPLES_MULITPLIER)    \n",
    "    elif strategy == COOCURRENCE_SAMPLING:\n",
    "        samples = get_coocurrence_sampling_item_ids(pid, n_samples*SAMPLES_MULITPLIER)\n",
    "    \n",
    "        #Completing the list of samples based on global popularity\n",
    "        if len(samples) < n_samples:\n",
    "            samples += get_popularity_sampling_item_ids(n_samples - len(samples))\n",
    "    else:\n",
    "        raise Exception('Not a valid strategy. Should be: (uniform|recency|popularity|cooccurrence)')\n",
    "        \n",
    "    #Removing repeated entries\n",
    "    samples = list(set(samples))\n",
    "    if ignore_list is not None:\n",
    "        samples = list([i for i in samples if i not in ignore_list])\n",
    "\n",
    "    return samples[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_candidate_samples_item_ids(63246, 10, strategy=COOCURRENCE_SAMPLING, ignore_list=[2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_item_ids(pids):\n",
    "    return items_df.loc[pids][['csid', 'ccid', 'bid', 'price', 'relative_price_to_avg_category', 'product_recency']] #\\\n",
    "    #.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#l= np.random.choice(items_df.index, 50)\n",
    "#l= np.ones(50, dtype='int')*10\n",
    "#l = [10] * 50\n",
    "#l = [33284, 6, 21, 2588, 23069, 1570, 8230, 552, 50, 4152, 55864, 24636, 113213, 65601, 9283, 1097, 1104, 90, 2651, 607, 13920, 7785, 619, 8821, 9337, 4221, 8330, 7307, 8848, 19089, 112784, 2197, 15002, 3234, 3252, 1207, 2745, 40128, 30918, 4299, 61646, 23247, 213, 6871, 8921, 15582, 6367, 15077, 8935, 3820, 1266, 1790, 256, 258, 3844, 1808, 14109, 1311, 8481, 3362, 1318, 2348, 301, 29996, 19245, 1331, 3891, 10553, 830, 1855, 1867, 3415, 2933, 8571, 21371, 390, 903, 1425, 404, 5527, 1958, 23477, 6071, 59321, 7100, 4545, 1479, 28628, 10200, 16857, 3544, 479, 480, 6628, 35812, 2032, 3059, 28153, 2046]\n",
    "#for i in range(100):\n",
    "#    for j in range(10):\n",
    "#        x = get_features_for_item_ids(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padarray(A, size):\n",
    "    if len(A) > size:\n",
    "        A = A[:size]\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padarray([1,2,3], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_samples(session_pids, user_past_pids, n_samples, strategy):\n",
    "    neg_samples_dict = defaultdict(list)\n",
    "    \n",
    "    #Ignores session items and also recently interacted items\n",
    "    ignore_ids = set(np.hstack([session_pids, user_past_pids]))\n",
    "    \n",
    "    for pid in session_pids:\n",
    "        if pid != 0:\n",
    "            #neg_item_ids = np.ones(50, dtype='int')*10\n",
    "            #neg_item_ids = [33284, 6, 21, 2588, 23069, 1570, 8230, 552, 50, 4152, 55864, 24636, 113213, 65601, 9283, 1097, 1104, 90, 2651, 607, 13920, 7785, 619, 8821, 9337, 4221, 8330, 7307, 8848, 19089, 112784, 2197, 15002, 3234, 3252, 1207, 2745, 40128, 30918, 4299, 61646, 23247, 213, 6871, 8921, 15582, 6367, 15077, 8935, 3820, 1266, 1790, 256, 258, 3844, 1808, 14109, 1311, 8481, 3362, 1318, 2348, 301, 29996, 19245, 1331, 3891, 10553, 830, 1855, 1867, 3415, 2933, 8571, 21371, 390, 903, 1425, 404, 5527, 1958, 23477, 6071, 59321, 7100, 4545, 1479, 28628, 10200, 16857, 3544, 479, 480, 6628, 35812, 2032, 3059, 28153, 2046]\n",
    "            neg_item_ids = get_candidate_samples_item_ids(pid, n_samples, \n",
    "                                                          ignore_list=ignore_ids,\n",
    "                                                         strategy=strategy\n",
    "                                                         )            \n",
    "            neg_item_features_dict = get_features_for_item_ids(neg_item_ids)\n",
    "\n",
    "            '''\n",
    "            pids_padded = padarray(neg_item_ids, n_samples).astype(int)\n",
    "            neg_samples_dict['sess_neg_pids'].append(pids_padded)\n",
    "            \n",
    "            for k, v in neg_item_features_dict.items():\n",
    "                values = padarray(v, n_samples)\n",
    "                values = values.astype(int) if k in ['csid', 'ccid', 'bid'] else values\n",
    "                neg_samples_dict['sess_neg_{}'.format(k)].append(values)\n",
    "            '''\n",
    "        '''\n",
    "        else:\n",
    "            #Creating padding neg samples for each padding interactions\n",
    "            missing_padding_neg_samples = len(session_pids) - len(neg_samples_dict['sess_neg_pids'])            \n",
    "            for p in range(missing_padding_neg_samples):\n",
    "                for k in neg_samples_dict:\n",
    "                    #Copying shape and dtype from the neg samples of the first interaction\n",
    "                    neg_samples_dict[k].append(np.zeros_like(neg_samples_dict[k][0]))\n",
    "        '''   \n",
    "    #Concatenating neg. samples of all session interactions because Petastorm data loader \n",
    "    #does not support lists of lists. It will require reshaping neg. samples features inside the Pytorch model\n",
    "    #for k in neg_samples_dict:  \n",
    "    #    neg_samples_dict[k] = np.hstack(neg_samples_dict[k])        \n",
    "\n",
    "    return neg_samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_neg_samples([10,20,30, 0, 0], n_samples=2, strategy='popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_rows_to_parquet(new_rows_df, path):\n",
    "    global pq_writer\n",
    "    new_rows_pa = pyarrow.Table.from_pandas(new_rows_df)\n",
    "    if pq_writer is None:\n",
    "        #Creating parent folder recursively\n",
    "        parent_folder = os.path.dirname(os.path.abspath(path))\n",
    "        if not os.path.exists(parent_folder):\n",
    "            os.makedirs(parent_folder)\n",
    "        #Creating parquet file\n",
    "        pq_writer = pq.ParquetWriter(path, new_rows_pa.schema) \n",
    "    pq_writer.write_table(new_rows_pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates neg. samples for all sessions and creates new parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "[Day 0] Loading sessions from parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc.parquet/session_start_date=2019-10-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches\n",
      "batch_id 0\n",
      "[Batch 0] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:16, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 1\n",
      "[Batch 1] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:42, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 2\n",
      "[Batch 2] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:07, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 3\n",
      "[Batch 3] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:40, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 4\n",
      "[Batch 4] Updating item stats\n",
      "[Batch 4] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [02:10, 26.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 5\n",
      "[Batch 5] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [02:30, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [02:53, 23.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [03:12, 22.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 8\n",
      "[Batch 8] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [03:35, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 9\n",
      "[Batch 9] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [03:57, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [04:21, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 11\n",
      "[Batch 11] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [04:40, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [04:59, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [05:17, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 14\n",
      "[Batch 14] Updating item stats\n",
      "[Batch 14] Appending new rows with neg samples to parquet: /home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [05:39, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [05:57, 19.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [06:20, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 17\n",
      "[Batch 17] Updating item stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [06:43, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 18\n"
     ]
    }
   ],
   "source": [
    "pq_writer = None\n",
    "\n",
    "reset_item_logs_and_statistics()\n",
    "try:\n",
    "    #For each file (day)\n",
    "    for idx_day, input_file in enumerate(input_parquet_files):\n",
    "        print('='*40)\n",
    "        print('[Day {}] Loading sessions from parquet: {}'.format(idx_day, input_file))\n",
    "        output_filename = get_output_path_parquet_neg_samples(input_file)\n",
    "        \n",
    "        if os.path.exists(output_filename):\n",
    "            raise Exception('Output parquet file already exists')\n",
    "        \n",
    "        #Loading parquet file and sorting sessions by timestamp\n",
    "        sessions_df = pd.read_parquet(input_file)\n",
    "        sessions_df.sort_values('session_start_ts', inplace=True)\n",
    "                \n",
    "        new_rows = []\n",
    "        \n",
    "        print('Processing batches')\n",
    "        #For each batch\n",
    "        for batch_id, batch in tqdm(enumerate(split_dataframe_into_chuncks_generator(sessions_df, \n",
    "                                                                                chunk_size = BATCH_SIZE))):\n",
    "            print('batch_id', batch_id)            \n",
    "            #For each row (session)\n",
    "            for i, row in batch.iterrows():\n",
    "                insert_update_session_items_metadata(row)\n",
    "                append_session_coocurrences_log(row)\n",
    "                \n",
    "                \n",
    "                #Ignoring first batch (not computing neg. samples nor saving to parquet)\n",
    "                if batch_id > 0:   \n",
    "                    #Generating neg. samples for each interaction in the session\n",
    "                    session_neg_samples_by_pid_dict = generate_neg_samples(row['sess_pid_seq'], \n",
    "                                                                           row['user_pid_seq_bef_sess'],\n",
    "                                                                           NUM_NEG_SAMPLES, \n",
    "                                                                           strategy=NEGATIVE_SAMPLING_STRATEGY\n",
    "                                                                          )\n",
    "                    #Merging user and session features with neg samples for the session\n",
    "                    new_row_with_neg_samples_dict = {**row.to_dict(), **session_neg_samples_by_pid_dict}\n",
    "                    new_rows.append(new_row_with_neg_samples_dict)\n",
    "                    \n",
    "            \n",
    "            #Each N batches updates item statistics (popularity, recency, co-occurrence)\n",
    "            #Ps. Do the update for all the first five batches of the first file , for better sampling\n",
    "            if (batch_id % BATCHES_TO_UPDATE_ITEM_STATS == BATCHES_TO_UPDATE_ITEM_STATS-1) or \\\n",
    "               (idx_day == 0 and batch_id < 5):\n",
    "                print('[Batch {}] Updating item stats'.format(batch_id))\n",
    "                remove_old_interactions(ITEM_STATS_KEEP_LAST_N_DAYS)\n",
    "                if NEGATIVE_SAMPLING_STRATEGY in [RECENT_POPULARITY_SAMPLING, COOCURRENCE_SAMPLING]:\n",
    "                    concat_sessions_coocurrences_log()\n",
    "                    update_items_coocurrences_counts()\n",
    "                    if RECENT_POPULARITY_SAMPLING:\n",
    "                        update_items_recent_popularity()\n",
    "                if NEGATIVE_SAMPLING_STRATEGY == RECENCY_SAMPLING:\n",
    "                    update_items_temporal_relevance_decay()\n",
    "                \n",
    "            #Each N batches appends the new rows with neg. samples to parquet file\n",
    "            if batch_id % BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES == BATCHES_TO_APPEND_ROWS_WITH_NEG_SAMPLES-1: \n",
    "                print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "                append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "                del(new_rows)\n",
    "                new_rows = []\n",
    "            \n",
    "               \n",
    "        #Save pending rows\n",
    "        if len(new_rows) > 0:\n",
    "            print('[Batch {}] Appending new rows with neg samples to parquet: {}'.format(batch_id,output_filename))\n",
    "            append_new_rows_to_parquet(pd.DataFrame(new_rows), output_filename)\n",
    "            del(new_rows)\n",
    "            new_rows = []\n",
    "            \n",
    "        #Flushing and releasing the current parquet file and proceeding for the new date\n",
    "        pq_writer.close()\n",
    "        pq_writer = None\n",
    "                \n",
    "        del(sessions_df)\n",
    "        gc.collect()\n",
    "        \n",
    "        break\n",
    "finally:\n",
    "    if pq_writer:\n",
    "        pq_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d4cbf4520b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitems_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'items_df' is not defined"
     ]
    }
   ],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_coocurrence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_coocurence_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items_recent_pop_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-899df3f6b875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitems_recent_pop_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'items_recent_pop_df' is not defined"
     ]
    }
   ],
   "source": [
    "items_recent_pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_temporal_relev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.DataFrame(columns={'pid': np.int64,\n",
    "                                 'csid': np.int32,\n",
    "                                 'ccid': np.int32,\n",
    "                                 'bid': np.int32,\n",
    "                                 'price': np.float,\n",
    "                                 'relative_price_to_avg_category': np.float,\n",
    "                                 'product_recency': np.float,\n",
    "                                 'first_ts': np.int,\n",
    "                                 'last_ts': np.int\n",
    "                                }).set_index('pid')\n",
    "    \n",
    "    items_coocurrence_df = pd.DataFrame(columns={'pid_a': np.int64, \n",
    "                                                 'pid_b': np.int64, \n",
    "                                                 'ts': np.int32, \n",
    "                                                 'count_flag': np.int16})\n",
    "    \n",
    "    items_coocurence_counts_df = None\n",
    "    items_recent_pop_df = None\n",
    "    items_temporal_relev_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the parquet with Negative samples with Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.pytorch import DataLoader\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.unischema import UnischemaField\n",
    "from petastorm.unischema import Unischema\n",
    "from petastorm.codecs import NdarrayCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_neg_parquet_path = 'file:///home/gmoreira/dataset/ecommerce_preproc_2019-10/ecommerce_preproc_neg_samples_50_strategy_cooccurrence.parquet/session_start_date=2019-10-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_schema_full = [\n",
    "  UnischemaField('user_idx', np.int, (), None, True),\n",
    "#   UnischemaField('user_session', str_, (), None, True),\n",
    "  UnischemaField('sess_seq_len', np.int, (), None, False),\n",
    "  UnischemaField('session_start_ts', np.int64, (), None, True),\n",
    "  UnischemaField('user_seq_length_bef_sess', np.int, (), None, False),\n",
    "  UnischemaField('user_elapsed_days_bef_sess', np.float, (), None, True),\n",
    "  UnischemaField('user_elapsed_days_log_bef_sess_norm', np.double, (), None, True),\n",
    "  UnischemaField('sess_pid_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etime_seq', np.int64, (None,), None, True),\n",
    "  UnischemaField('sess_etype_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_csid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_ccid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_bid_seq', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_price_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_dtime_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_product_recency_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_relative_price_to_avg_category_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_hour_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_month_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofweek_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_sin_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_et_dayofmonth_cos_seq', np.float, (None,), None, True),\n",
    "  UnischemaField('user_pid_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etime_seq_bef_sess', np.int64, (None,), None, True),\n",
    "  UnischemaField('user_etype_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_csid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_ccid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_bid_seq_bef_sess', np.int, (None,), None, True),\n",
    "  UnischemaField('user_price_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_dtime_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_product_recency_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_relative_price_to_avg_category_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_hour_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_month_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofweek_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_sin_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('user_et_dayofmonth_cos_seq_bef_sess', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_pids', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_csid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_ccid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_bid', np.int, (None,), None, True),\n",
    "  UnischemaField('sess_neg_price', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_relative_price_to_avg_category', np.float, (None,), None, True),\n",
    "  UnischemaField('sess_neg_product_recency', np.float, (None,), None, True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DataLoader(\n",
    "    make_batch_reader(input_with_neg_parquet_path, \n",
    "                num_epochs=1,\n",
    "                # transform_spec=transform\n",
    "                schema_fields=recsys_schema_full,\n",
    "    ), batch_size=2) as train_loader:\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(\"i:{}\".format(i))\n",
    "        print(batch)\n",
    "        print(batch['sess_neg_product_recency'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Identify bottleneck as items_df and items_coocurrence_df become larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
