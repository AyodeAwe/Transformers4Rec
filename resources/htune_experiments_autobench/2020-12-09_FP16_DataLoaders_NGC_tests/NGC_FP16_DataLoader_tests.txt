Baseline

Id: 1569300
05:29:02

chocolate-leaf-2427
https://app.wandb.ai/gspmoreira/huggingface/runs/gjn8q24b


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash gpt2-fp16 full_noneg session_cooccurrence --data_loader_engine "pyarrow" --workers_count 0 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


----------------------------------------------------------------------------------------

PyArrow Data Loader using 8 workers + FP32

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --data_loader_engine "pyarrow" --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635488
03:19:54


https://wandb.ai/gspmoreira/huggingface/runs/10b837ul?workspace=user-


----------------------------------------------------------------------------------------

PyArrow Data Loader using 8 workers + FP16 (Native AMP)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine "pyarrow" --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635489
03:19:08 (No TensorCore usage with Native AMP. Not really using FP16 for this image)

wobbly-planet-2858
https://app.wandb.ai/gspmoreira/huggingface/runs/1mi09dd8

-------------------------------

Changing the way we collect the fp16 param, adding logging and running again with Native AMP

Id: 1635657
03:17:53 (No TensorCore usage with Native AMP. Not really using FP16 for this image)

leafy-feather-2859
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/4mdahebl



-------------------------------
PyArrow Data Loader using 8 workers + FP16 (APEX)

Enforcing APEX usage in the commit to test


Id: 1635691
02:48:10 (Now using TensorCores)

dainty-river-2861
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/34plexr9

-------------------------------

PyArrow Data Loader using 1 workers + FP16 (APEX)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine "pyarrow" --workers_count 1 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1636741
03:58:45

-------------------------------

PyArrow Data Loader using 0 workers + FP16 (APEX)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine "pyarrow" --workers_count 0 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1636744
05:56:28


=======================================================================================================================

Experiments with the RAPIDS-based image

PyArrow + FP32

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --data_loader_engine pyarrow --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635959
05:26:35

---------------------------------------------------

PyArrow + FP16 (APEX)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine pyarrow --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1636749
05:16:25

---------------------------------------------------

PyArrow + FP16 (Native AMP)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine pyarrow --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635987
04:40:40


-----------------------

Running again with Native AMP, after installing APEX in the image

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine pyarrow --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1636762
04:37:49

---------------------------------------------------

NVTabular + FP32

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635990
02:44:15

---------------------------------------------------

NVTabular + FP16 (APEX)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1636752
FAILED (OOM on cudf on day 10 (epoch 6).

----------------

Running it again with --nvt_part_mem_fraction 0.2

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --nvt_part_mem_fraction 0.2 --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1636839
02:51:18

---------------------------------------------------

NVTabular + FP16 (APEX) + Dataset with fixed row group sizes

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

Id: 1636841
FAILED


---------------------------------------------------

NVTabular + FP16 (Native AMP)

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

ID: 1635993
02:34:33

---------------------------------------------------

NVTabular + FP16 + Dataset with fixed row group sizes

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

ID: 1636047
02:30:55

----------------------------

Run this experiment again, with the NVT Data Loader changed to support sequence features with different lengths for different examples, to see if the performance drops a lot

With --per_device_train_batch_size 512

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

Id: 1644741
02:13:51 (It improved)


With --per_device_train_batch_size 256

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 256 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

Id: 1644743
03:04:41 (Slower with smaller batch size, as expected)



-----------------------------------------

Run with 1 GPU only now




With --per_device_train_batch_size 512

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.1.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

Id: 1645059
ERROR - OOM


With --per_device_train_batch_size 256

ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.1.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 256 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data

Id: 1645062
02:30:38


--------------------------

Changed NVT dataloader to use as many GPU as available


Run again with 2 GPUs and --per_device_train_batch_size 512


ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data


Id: 1645528
02:15:43

------


Run with 4 GPUs and --per_device_train_batch_size 1024


ngc batch run --name "tranf4rec-fp16-dataloaders-tests" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.4.norm --commandline "bash -c 'nvidia-smi && source activate rapids && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash tranf4rec-fp16-dataloaders-tests full_noneg session_cooccurrence --fp16 --data_loader_engine nvtabular --workers_count 8 --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date'" --result /results --image "nvidian/prj-recsys/transf4rec_nvt_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 70490:/data


Id: 1645530
02:58:10 (slower than with two GPUs!)
