Baselines

Best GPT-2: magic-firefly-2302
Job Id: 1563071
https://wandb.ai/gspmoreira/huggingface/runs/3aqddi9a/overview
AOD_all_Test_ndcg@1000_all:  0.2734

-------------

With --shuffle_buffer_size 10240

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 --shuffle_buffer_size 100 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1569309

INVESTIGATE BETTER THE JOB TO CHECK WHY IT FAILS WITH SHUFFLING
https://ngc.nvidia.com/jobs/1569309/log

Running it again with --shuffle_buffer_size 100:
Id: 1582109

Without shuffle
1582168



-------------

With batch_size from --per_device_train_batch_size 192 to 320 (no shuffling)

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 320 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1567768

SUCEEDED

-------------

With batch_size from --per_device_train_batch_size 192 to 384 (no shuffling)


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 384 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1567777

SUCEEDED


wandb: Syncing run avid-waterfall-2395
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1xs63kiw

From 7h -> 5h43 
AOD_all_Test_ndcg@1000_all: 0.273


-------------

With batch_size from --per_device_train_batch_size 192 to 512 (no shuffling)


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 512 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1569300

SUCEEDED


From 7h -> 5h29
wandb: Syncing run chocolate-leaf-2427
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/gjn8q24b


AOD_all_Test_ndcg@1000_all: 0.2721

-------------

With batch_size from --per_device_train_batch_size 192 to 1024 (no shuffling)


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 1024 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

 Id: 1568130

FAIL
File "/workspace/recsys/transformers4recsys/codes/recsys_trainer.py", line 456, in _training_step
loss.backward()
File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 185, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 127, in backward
allow_unreachable=True) # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 3.34 GiB (GPU 0; 31.75 GiB total capacity; 21.04 GiB already allocated; 2.29 GiB free; 28.45 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call

=============================================================================


Best XLNET bi: fresh-microwave-1910
Job Id: 1545068
https://wandb.ai/gspmoreira/huggingface/runs/3h57d4we?workspace=
AOD_all_Test_ndcg@1000_all:   0.2758

-------------

With --shuffle_buffer_size 10240

ngc batch run --name "tranf4rec-htune-v08-xlnet-bi-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-xlnet-bi-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type xlnet --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --attn_type bi --mlm --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.0005056660668603287 --learning_rate_warmup 0.0005308343307929519 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup linear_with_warmup --dropout 0.0 --weight_decay 9.011290200865024e-06 --d_model 320 --n_layer 3 --n_head 4 --mlm_probability 0.5 --shuffle_buffer_size 10240  && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


==========================================================================================================



Tests with AMP - Automatic Mixed Precision

Baseline
magic-firefly-2302
https://wandb.ai/gspmoreira/huggingface/runs/3aqddi9a/overview
AOD_all_Test_ndcg@1000_all:  0.2734


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 --fp16 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1587523

wandb: Syncing run frosty-deluge-2658
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/2mc9yte0



------------------------------------------------------------------

Test with --fp16 and --max_seq_len 17 (so that shifted sequences become 16 length, which is multiple of 8, to check possible speed-ups with TensorCores)


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1587656


wandb: Syncing run brisk-butterfly-2665
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/2h499tui

------------------------------------------------------------------

Test with --fp16 and --max_seq_len 17 (so that shifted sequences become 16 length, which is multiple of 8, to check possible speed-ups with TensorCores)
and with --n_head 1 (to see if that dimension is ignored as the others are multiples of 8)


ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1587665


wandb: Syncing run rural-surf-2667
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/2xtnz2uh


------------------------------------------------------------------

Test with --max_seq_len 17 --n_head 2 but not with --fp16

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 192 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 2 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1587659

wandb: Syncing run still-bush-2666
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1taqe0qd


===========================================================

Trying larger batch sizes with --fp16


With  --per_device_train_batch_size 1024

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 1024 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1587680 (OOM after 1 min)


FAILED


With  --per_device_train_batch_size 768

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 768 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1587692

wandb: Syncing run different-gorge-2669
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/3n04ol9w

Failed (OOM after 43 min)


With  --per_device_train_batch_size 896

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 896 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1587707

Failed (OOM after 3 min)

------------------------------------------

With  --per_device_train_batch_size With  --per_device_train_batch_size 640 --fp16

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 640 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --fp16 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1588109

wandb: Syncing run fiery-sound-2671
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/3hvc39yh

Runtime: 04:59:46

Finished successfully



-----------------------------------------------


With  --per_device_train_batch_size With  --per_device_train_batch_size 640 WITHOUT --fp16

ngc batch run --name "tranf4rec-htune-v08-gpt2-shuffle" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2-shuffle full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50  --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 10 --per_device_train_batch_size 640 --learning_rate 0.00014969647714359603 --learning_rate_warmup 0.00014529247619095396 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 6.211639773976265e-05 --d_model 320 --n_layer 1 --n_head 1 --max_seq_len 17 && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1593254

wandb: Syncing run stilted-darkness-2672
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/12599z8c

RUNNING
