Starting a small machine to run AutoBench


ngc batch run --name "transf4rec-autobench-htune" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.16g.1.norm --commandline "apt update; apt-get install screen -y; pip install -r requirements.txt --quiet; git pull origin experimentation & date & jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --notebook-dir=/ --NotebookApp.allow_origin='*' & date; sleep 672h" --total-runtime 672h --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --port 8888 --port 8887 --workspace gmoreira-wksp:/wksp:RW


Commands in the machine:
echo "export PATH=\"\$PATH:/wksp/ngc/\"" >> ~/.bash_profile && source ~/.bash_profile
mkdir /root/.ngc ; cp /wksp/ngc/config /root/.ngc/config

cd /wksp/autobench
pip install -r requirements.txt 


Job Ids
1st: 1536764
2nd: 1551217
3rd: 1571838



-----------------------------------------

ALL FEATURES


#Hypertuning job - ALL FEATURES - XLNET (uni)

screen -S xlnet_uni

source ~/.bash_profile

mkdir -p /results/xlnet-uni/htune_log
python main.py --optuna_study_name htune-v08-xlnet-uni --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/xlnet-uni/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/xlnet_uni/tranf4rec-htune-v08-xlnet-uni.yaml"


[then, ctrl + A + D: make me outside of the screen session]


Best XLNET uni: exalted-firefly-2011
AOD_all_Test_ndcg@1000_all:   0.2703



---------------------------------------


#Hypertuning job - ALL FEATURES - XLNET (bi)

screen -S xlnet_bi

source ~/.bash_profile

mkdir -p /results/xlnet-bi/htune_log
python main.py --optuna_study_name htune-v08-xlnet-bi --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/xlnet-bi/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/xlnet_bi/tranf4rec-htune-v08-xlnet-bi.yaml"

[then, ctrl + A + D: make me outside of the screen session]



Best XLNET bi: fresh-microwave-1910
https://wandb.ai/gspmoreira/huggingface/runs/3h57d4we?workspace=
AOD_all_Test_ndcg@1000_all:   0.2758



---------------------------------------



#Hypertuning job - ALL FEATURES - GPT-2

screen -S gpt2

source ~/.bash_profile

mkdir -p /results/gpt2/htune_log
python main.py --optuna_study_name htune-v08-gpt2 --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/gpt2/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/gpt2/tranf4rec-htune-v08-gpt2.yaml"


[then, ctrl + A + D: make me outside of the screen session]


major-morning-2338
https://wandb.ai/gspmoreira/huggingface/runs/ylnjbk9r/overview?workspace=user-gspmoreira
AOD_all_Test_ndcg@1000_all:  0.2735


magic-firefly-2302
https://wandb.ai/gspmoreira/huggingface/runs/3aqddi9a/overview
AOD_all_Test_ndcg@1000_all
AOD_all_Test_ndcg@1000_all:  0.2734

----------------

Running again the best GPT-2 (major-morning-2338), but now with --log_predictions --log_attention_weights

ngc batch run --name "tranf4rec-htune-08-gpt2" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v08-gpt2 full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 1.25 --hidden_act gelu_new --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --eval_on_last_item_seq_only --warmup_days 1 --num_train_epochs 9 --per_device_train_batch_size 192 --learning_rate 0.00021103662821025438 --learning_rate_warmup 0.00020003190020791603 --learning_rate_schedule linear_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 3.522219206555709e-05 --d_model 320 --n_layer 1 --n_head 1 --log_predictions --log_attention_weights && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1567795

wandb: Syncing run devout-yogurt-2400
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1xtk19yc


---------------------------------------

ONLY PID


#Hypertuning job - ONLY PID - XLNET (uni)

screen -S xlnet_uni_pid

source ~/.bash_profile

mkdir -p /results/xlnet-uni-pid/htune_log
python main.py --optuna_study_name htune-v08-xlnet-uni-pid --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/xlnet-uni-pid/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/xlnet_uni_onlypid/tranf4rec-htune-v08-xlnet-uni-pid.yaml"


[then, ctrl + A + D: make me outside of the screen session]


RUNNING 


---------------------------------------


#Hypertuning job - ONLY PID - XLNET (bi)

screen -S xlnet_bi_pid

source ~/.bash_profile

mkdir -p /results/xlnet-bi-pid/htune_log
python main.py --optuna_study_name htune-v08-xlnet-bi-pid --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/xlnet-bi-pid/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/xlnet_bi_onlypid/tranf4rec-htune-v08-xlnet-bi-pid.yaml"

[then, ctrl + A + D: make me outside of the screen session]


Best model
warm-leaf-2243
https://wandb.ai/gspmoreira/huggingface/runs/i9o75j9u?workspace=user-gspmoreira

AOD_all_Test_ndcg@1000_all: 0.2549


---------------------------------------



#Hypertuning job - ONLY PID - GPT-2

screen -S gpt2_pid_pid

source ~/.bash_profile

mkdir -p /results/gpt2-pid/htune_log
python main.py --optuna_study_name htune-v08-gpt2-pid --concurrent_jobs 5 --optuna_num_trials 100 --verbose --optuna_log_dir "/results/gpt2-pid/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v08_eval_on_last_item_new_autobench/gpt2_onlypid/tranf4rec-htune-v08-gpt2-pid.yaml"


[then, ctrl + A + D: make me outside of the screen session]


RUNNING


