GPT-2

Investigating why fallen-field-1434 accuracy is so different from daily-paper-1619, even though the later used --eval_on_last_item_seq_only

Best model find in hypertuning

fallen-field-1434 (gelu_new)
https://wandb.ai/gspmoreira/huggingface/runs/1j4rgno5/overview?workspace=user-


Reproducing with the same hyperparameters (learning rate hparams were split between warmup and fine tuning, but as schedule is constant and LR values are the same it should not make difference)


---------------------------------------------

--hidden_act gelu

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --log_predictions --log_attention_weights" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1541726


wandb: Syncing run snowy-sun-1820
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1zk71iin


---------------------------------------------------


--hidden_act gelu_new

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --log_predictions --log_attention_weights" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1541731

wandb: Syncing run floral-bird-1821
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1v49o42p

-------------------------------------------------------

--hidden_act gelu_new --eval_on_last_item_seq_only

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --eval_on_last_item_seq_only --log_predictions --log_attention_weights" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1541732

wandb: Syncing run devoted-serenity-1822
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/3smf9zg2


==================================================================================


Running Best GPT-2 with different random seeds

SEED 42 

Shared with above
Id: 1541731

--------------
SEED 1

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --seed 1" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data


Id: 1541736


wandb: Syncing run northern-planet-1823
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/1khoo5qr


-----------------------


SEED 2

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --seed 2" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1541737


wandb: Syncing run vivid-salad-1824
wandb: ðŸš€ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/2zl1v6ne


----------------------

SEED 3

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --seed 3" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1541738

----------------------

SEED 4

ngc batch run --name "htune-v07-gpt2-debug" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gpt2-debug full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --seed 4" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1541739



