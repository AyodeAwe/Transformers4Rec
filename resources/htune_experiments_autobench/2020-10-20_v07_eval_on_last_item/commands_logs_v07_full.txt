Starting a small machine to run AutoBench


ngc batch run --name "transf4rec-autobench-htune" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.16g.1.norm --commandline "apt update; apt-get install screen -y; pip install -r requirements.txt --quiet; git pull origin experimentation & date & jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --notebook-dir=/ --NotebookApp.allow_origin='*' & date; sleep 672h" --total-runtime 672h --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --port 8888 --port 8887 --workspace gmoreira-wksp:/wksp:RW


Commands in the machine:
echo "export PATH=\"\$PATH:/wksp/ngc/\"" >> ~/.bash_profile && source ~/.bash_profile
mkdir /root/.ngc ; cp /wksp/ngc/config /root/.ngc/config

cd /wksp/autobench
pip install -r requirements.txt 


Last Job id: 1536764


-----------------------------------------


#Hypertuning job - XLNET (uni)

screen -S xlnet_uni

source ~/.bash_profile

python main.py --htune_study_name htune-v07-xlnet-uni --concurrent_jobs 5 --htune_num_trials 100 --verbose --htune_log_dir "/results/xlnet-uni/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v07_eval_on_last_item/xlnet_uni/tranf4rec-htune-v07-xlnet-uni.yaml"


[then, ctrl + A + D: make me outside of the screen session]



---------------------------------------


#Hypertuning job - XLNET (bi)

screen -S xlnet_bi

source ~/.bash_profile

python main.py --htune_study_name htune-v07-xlnet-bi --concurrent_jobs 5 --htune_num_trials 100 --verbose --htune_log_dir "/results/xlnet-bi/htune_log" "/workspace/recsys/transformers4recsys/resources/htune_experiments_autobench/2020-10-20_v07_eval_on_last_item/xlnet_bi/tranf4rec-htune-v07-xlnet-bi.yaml"

[then, ctrl + A + D: make me outside of the screen session]



---------------------------------------


GPT-2 - Manually running model with the best hparams for v06 hypertuning (but now with gelu_new and --eval_on_last_item_seq_only and dropout fixed)

ngc batch run --name "htune-v07-gpt2" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-07-gpt2 full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gpt2 --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --hidden_act gelu_new --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 9 --per_device_train_batch_size 64 --warmup_days 1 --learning_rate 0.002162094632215143 --learning_rate_warmup 0.002162094632215143 --learning_rate_schedule constant_with_warmup --learning_rate_schedule_warmup constant_with_warmup --dropout 0.1 --weight_decay 5.189724812073076e-06 --d_model 256 --n_layer 2 --n_head 2 --eval_on_last_item_seq_only --log_predictions --log_attention_weights" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1536845






P.s. compare to the following GPT-2 experiment with --eval_on_last_item_seq_only (before fixing dropout)
wandb: Syncing run golden-jazz-1621
wandb: üöÄ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/stmvcew1

---------------------------------------

GRU - Manually running model with the best hparams for v06 hypertuning (now with --eval_on_last_item_seq_only)

ngc batch run --name "htune-v07-gru" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v07-gru full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type gru --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 10 --per_device_train_batch_size 256 --warmup_days 1 --learning_rate 0.0002024939202283675 --learning_rate_warmup 0.0002024939202283675 --learning_rate_schedule  constant_with_warmup --learning_rate_schedule_warmup  constant_with_warmup --dropout 0.2 --weight_decay 9.065312473196671e-05 --d_model 320 --n_layer 1 --eval_on_last_item_seq_only --log_predictions && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1536831


wandb: Syncing run royal-sponge-1648
wandb: ‚≠êÔ∏è View project at https://app.wandb.ai/gspmoreira/huggingface
wandb: üöÄ View run at https://app.wandb.ai/gspmoreira/huggingface/runs/ok77w9cp

---------------------------------------

AVGSEQ - Manually running model with the best hparams for GRU's v06 hypertuning (now with --eval_on_last_item_seq_only)

ngc batch run --name "htune-v07-avgseq" --preempt RUNONCE --ace nv-us-west-2 --instance dgx1v.32g.2.norm --commandline "nvidia-smi && wandb login 76eea90114bb1cdcbafe151b262e4a5d4ff60f12 && date && git pull origin experimentation && date && bash script/run_transformer_v2.bash htune-v06-avgseq full_noneg session_cooccurrence --start_date 2019-10-01 --end_date 2019-10-15 --model_type avgseq --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --all_rescale_factor 1.0 --neg_rescale_factor 0.0 --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_num_cosine_cycles 4.0 --dataloader_drop_last --compute_metrics_each_n_steps 50 --max_seq_len 20 --num_train_epochs 10 --per_device_train_batch_size 256 --warmup_days 1 --learning_rate 0.0002024939202283675 --learning_rate_warmup 0.0002024939202283675 --learning_rate_schedule  constant_with_warmup --learning_rate_schedule_warmup  constant_with_warmup --dropout 0.2 --weight_decay 9.065312473196671e-05 --d_model 320 --n_layer 1 --eval_on_last_item_seq_only --log_predictions && date" --result /results --image "nvidian/prj-recsys/transf4rec_exp:0.1.0" --org nvidian --team prj-recsys --datasetid 66609:/data

Id: 1536832








